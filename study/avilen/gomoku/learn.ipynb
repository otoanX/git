{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"learn.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LsYIUMUnHtCZ"},"source":["# 強化学習\n","\n","学習部のソースコード\n","\n","## 概要\n","\n","1. 五目並べのAI同士の対戦によって教師データを生成します。\n","1. 生成された教師データを回転、反転し、盤面数を増やします。\n","1. すべての盤面に報酬（評価値）を与えます。\n","1. 教師データによってCNNを最適化し、AIを更新します。\n","1. 再び対戦し、1〜5を繰り返します"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X2xW43AaHtCa","colab":{}},"source":["import numpy as np\n","import random as rd\n","import copy\n","from time import time\n","from gomoku import *\n","from value_network import *\n","from ai import *\n","import json"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A_uy7oKYHtCf","colab":{}},"source":["size = 9 # ボードサイズ\n","n_learn = 100 # 学習サイクル数\n","n_epoch = 10 # バッチごとの学習数\n","batchsize = 128 # ミニバッチサイズ\n","gamenumber = 500 # 学習サイクルごとの対戦数\n","rate = 1e-5 # 学習率\n","model = value_network_model(size, rate) # モデル"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aPlxLjJOHtCh","colab":{}},"source":["# 学習\n","for learn in range(n_learn):\n","    x_train = [] # 教師データ（盤面）の格納先\n","    t_train = [] # 教師データ（報酬）の格納先\n","    start = time() # 開始時間\n","    print('learn %d' % learn)\n","    \n","    # 自己対戦で教師データを作成\n","    for i in range(gamenumber):\n","        g = Game() # 新しい五目並べゲーム\n","        g_history = [] # 棋譜の格納先\n","        win = 0 # 勝者\n","        \n","        # 一回の対戦\n","        for i in range(size*size):\n","#             ランダムに石を置く\n","            g.rand_put() # ランダムに石を置く\n","#             AIで石を置く(初期の学習では使用せず、学習が一定以上で行う)\n","#             g = ai_put(g, model, random=10.)\n","            win = g.end_game() # ゲームの終了判定\n","            g_history.append(copy.deepcopy(g))\n","            if win != 0:\n","                break # ゲームが終了していればループから出る\n","        \n","        \n","        g_temp = copy.deepcopy(g_history[:])\n","        \n","#         # 回転、反射対象な盤面を生成(ai_putを使用するときの盤面のオーグメント)\n","#         for _ in range(3):\n","#             for g_h in g_temp:\n","#                 g_h.rotate()\n","#                 g_history.append(copy.deepcopy(g_h))\n","#         for g_h in g_temp:\n","#             g_h.reflect()\n","#         for _ in range(4):\n","#             for g_h in g_temp:\n","#                 g_history.append(copy.deepcopy(g_h))\n","#                 g_h.rotate()\n","        \n","        # 学習ラベルを生成\n","        for g_h in g_history:\n","            # 報酬（黒勝利：1.0, 白勝利：-1.0, 引き分け：0）\n","            q_value = [0.0] if win is 0 else [1.0] if win is 1 else [-1.0]\n","            x_train.append(g_h.square)\n","            t_train.append(q_value)\n","\n","    ave_loss = 0\n","    # 最適化(学習)\n","    print('boards: ', len(x_train))\n","    for epoch in range(n_epoch):\n","        print('epoch %d | ' % epoch, end='')\n","        # 0~len(x_train)-1の自然数をランダムに並べ替えたリストを生成\n","        perm = np.random.permutation(len(x_train))\n","        loss = 0\n","        for i in range(0, len(x_train), batchsize):\n","            x_batch = [x_train[j] for j in perm[i:i+batchsize]]\n","            t_batch = [t_train[j] for j in perm[i:i+batchsize]]\n","            # loss.append(0.1)\n","            loss += model.optimize(x_batch, t_batch)\n","        loss /= int(len(x_train) / batchsize)\n","        print(\"loss {0:.3f}\".format(loss))\n","        ave_loss += loss\n","    ave_loss /= n_epoch\n","    end = time()\n","    print(\"time:{0:.1f}\".format(end - start))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RM5OhZRqHtCk","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}