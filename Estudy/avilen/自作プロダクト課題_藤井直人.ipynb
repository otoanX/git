{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取り組む課題「2つの重なった手書き数字の識別」\n",
    "まずは重なった手書き数字を作る。  \n",
    "これはなんとか作成できた。  \n",
    "(最初は配列を合計して2で割っていたが、文字が薄くなったので、np.maximum関数で大きい数字をとるようにした。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#いつものやつ\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "# 乱数シードを指定\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnistの準備\n",
    "if os.path.exists('mnist_784'):\n",
    "    with open('mnist_784','rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "else:\n",
    "    mnist = datasets.fetch_openml('mnist_784')\n",
    "    with open('mnist_784', 'wb') as f:\n",
    "        pickle.dump(mnist, f)\n",
    "# 画像とラベルを取得\n",
    "X, T = mnist.data, mnist.target\n",
    "# 訓練データとテストデータに分割\n",
    "x_train, x_test, t_train, t_test = train_test_split(X, T, test_size=0.2)\n",
    "\n",
    "# ラベルデータをint型にし、one-hot-vectorに変換します\n",
    "t_train = np.eye(10)[t_train.astype(\"int\")]\n",
    "t_test = np.eye(10)[t_test.astype(\"int\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAANaElEQVR4nO3df6hc9ZnH8c/HbIOYVklWc43JZW1jBG/UtUsQxbJmrS0qQizokiCSsrLpHxUSUFhx/2hgKYps1cU/CrdEmi5dS1GjoRSbGMpmFyEkalaTpq3ZEHtvckk2xCQ2/ojGZ/+4J3KNd87czDkzZ5Ln/YLLzJxnzpyHIZ98z5kzc76OCAE4953XdAMAeoOwA0kQdiAJwg4kQdiBJP6ilxuzzUf/QJdFhCdbXmlkt32b7T/Y3m374SqvBaC73Ol5dtvTJP1R0rckjUraKmlZRPyuZB1GdqDLujGyXy9pd0TsiYgTkn4haUmF1wPQRVXCPlfSyITHo8Wyz7G9wvY229sqbAtARVU+oJtsV+ELu+kRMSxpWGI3HmhSlZF9VNLghMfzJO2v1g6AbqkS9q2SFtj+qu3pkpZKWl9PWwDq1vFufER8YvsBSb+RNE3SMxGxs7bOANSq41NvHW2MY3ag67rypRoAZw/CDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuh4ymagqueee660fvPNN5fW77vvvtL6yy+/fMY9ncsqhd32XknvSTop6ZOIWFRHUwDqV8fI/ncRcaiG1wHQRRyzA0lUDXtI2mD7NdsrJnuC7RW2t9neVnFbACqouht/U0Tstz1b0kbbv4+IzROfEBHDkoYlyXZU3B6ADlUa2SNif3F7UNI6SdfX0RSA+nUcdtszbH/l1H1J35a0o67GANTLEZ3tWdv+msZHc2n8cOA/IuKHbdZhNz6Z885rPZ688847pesODAyU1sfGxkrrQ0NDLWvHjx8vXfdsFhGebHnHx+wRsUfSX3fcEYCe4tQbkARhB5Ig7EAShB1IgrADSfATV3TVPffc07I2d+7c0nVXrlxZWn/66ac76ikrRnYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSILz7Oiqu+++u+N1T5w4UWMnYGQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ6vpR0RxvjUtJdMW3atJa1iy66qHTdw4cP193O5+zZs6dlbd68eaXrDg4OltYPHDjQUU/nulaXkmZkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk+D37OWDNmjUta7fffnvpugsXLiytHzp0qLQ+e/bs0vqFF17YsrZly5bSdTmPXq+2I7vtZ2wftL1jwrJZtjfafru4ndndNgFUNZXd+J9Kuu20ZQ9L2hQRCyRtKh4D6GNtwx4RmyWd/p3KJZLWFvfXSrqr5r4A1KzTY/aBiBiTpIgYs93ywM32CkkrOtwOgJp0/QO6iBiWNCzxQxigSZ2eejtge44kFbcH62sJQDd0Gvb1kpYX95dLeqmedgB0S9vdeNvPSlos6WLbo5J+IOkxSb+0fb+kP0lqPQk3uu6qq65qWbvkkktK173ssstK6+3Os1955ZWl9VmzZrWs7d69u3Rd1Ktt2CNiWYvSN2vuBUAX8XVZIAnCDiRB2IEkCDuQBGEHkuAnrmeB+fPnl9aHhoZa1kZGRkrXrXr6a9WqVaV1e9KrGkuSnnrqqUrbxplhZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJDjPfhZodz56xowZLWsvvvhi6brvv/9+Rz2dcu2115bWy6YE7/Z00fg8RnYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSILz7H2g3e/VFy9eXFr/+OOPW9YeffTRTlr6zMDAQGm97FLRkvTuu++2rH3wwQcd9YTOMLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKcZ+8DDz30UGm97PfqknTkyJGWtaVLl5aue+utt5bWq55nL/Pqq6+W1su+PzAVN954Y8vasWPHKr322ajtyG77GdsHbe+YsGy17X22txd/d3S3TQBVTWU3/qeSbptk+ZMRcV3x9+t62wJQt7Zhj4jNkrh+EHCWq/IB3QO23yx282e2epLtFba32d5WYVsAKuo07D+WNF/SdZLGJP2o1RMjYjgiFkXEog63BaAGHYU9Ig5ExMmI+FTSTyRdX29bAOrWUdhtz5nw8DuSdrR6LoD+4LLrekuS7WclLZZ0saQDkn5QPL5OUkjaK+l7ETHWdmN2+cbOUYsWlR/BbN68ubR+/vnn19lOT+3cubNlbd26daXrtvu9+xtvvFFa37hxY8vayZMnS9c9m0WEJ1ve9ks1EbFsksVrKncEoKf4uiyQBGEHkiDsQBKEHUiCsANJtD31VuvGkp56e+WVV0rrt9xyS6XXLzu91W7K5nan9R588MHS+r59+0rrV199dcva0aNHS9dFZ1qdemNkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkuJR0DS644ILS+oIFC0rr7S6ZvHr16tL6E0880bL20Ucfla47NDRUWm93nn3Dhg2ldc6l9w9GdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgvPsNfjwww9L6+vXry+tj4yMlNYff/zxM+5pqu68885K6z/55JM1dYJuY2QHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSS4bnxyW7duLa1feumlpfVrrrmmtH7kyJEz7gnVdHzdeNuDtn9re5ftnbZXFstn2d5o++3idmbdTQOoz1R24z+R9GBEXCXpBknftz0k6WFJmyJigaRNxWMAfapt2CNiLCJeL+6/J2mXpLmSlkhaWzxtraS7utUkgOrO6Lvxti+X9HVJWyQNRMSYNP4fgu3ZLdZZIWlFtTYBVDXlsNv+sqTnJa2KiGP2pJ8BfEFEDEsaLl6DD+iAhkzp1JvtL2k86D+PiBeKxQdszynqcyQd7E6LAOrQdmT3+BC+RtKuiJh4zeL1kpZLeqy4fakrHaKrpk+fXlpvd6loTq2dPaayG3+TpPskvWV7e7HsEY2H/Je275f0J0n3dKdFAHVoG/aI+G9JrQ7Qv1lvOwC6ha/LAkkQdiAJwg4kQdiBJAg7kASXkj7HXXHFFaX1hQsXltbvvffeOttBgxjZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJzrOf42644YbS+vHjx0vrR48erbMdNIiRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dz7OW5wcLC0Pjo6WlofGRmpsx00iJEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JwRJQ/wR6U9DNJl0r6VNJwRPyb7dWS/lHS/xVPfSQift3mtco3BqCyiJh01uWphH2OpDkR8brtr0h6TdJdkv5e0p8j4l+n2gRhB7qvVdinMj/7mKSx4v57tndJmltvewC67YyO2W1fLunrkrYUix6w/abtZ2zPbLHOCtvbbG+r1CmAStruxn/2RPvLkv5T0g8j4gXbA5IOSQpJ/6LxXf1/aPMa7MYDXdbxMbsk2f6SpF9J+k1EPDFJ/XJJv4qIq9u8DmEHuqxV2Nvuxtu2pDWSdk0MevHB3SnfkbSjapMAumcqn8Z/Q9J/SXpL46feJOkRScskXafx3fi9kr5XfJhX9lqM7ECXVdqNrwthB7qv4914AOcGwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBK9nrL5kKR3Jjy+uFjWj/q1t37tS6K3TtXZ21+1KvT09+xf2Li9LSIWNdZAiX7trV/7kuitU73qjd14IAnCDiTRdNiHG95+mX7trV/7kuitUz3prdFjdgC90/TIDqBHCDuQRCNht32b7T/Y3m374SZ6aMX2Xttv2d7e9Px0xRx6B23vmLBslu2Ntt8ubiedY6+h3lbb3le8d9tt39FQb4O2f2t7l+2dtlcWyxt970r66sn71vNjdtvTJP1R0rckjUraKmlZRPyup420YHuvpEUR0fgXMGz/raQ/S/rZqam1bD8u6XBEPFb8RzkzIv6pT3pbrTOcxrtLvbWaZvy7avC9q3P68040MbJfL2l3ROyJiBOSfiFpSQN99L2I2Czp8GmLl0haW9xfq/F/LD3Xore+EBFjEfF6cf89SaemGW/0vSvpqyeaCPtcSSMTHo+qv+Z7D0kbbL9me0XTzUxi4NQ0W8Xt7Ib7OV3babx76bRpxvvmvetk+vOqmgj7ZFPT9NP5v5si4m8k3S7p+8XuKqbmx5Lma3wOwDFJP2qymWKa8eclrYqIY032MtEkffXkfWsi7KOSBic8nidpfwN9TCoi9he3ByWt0/hhRz85cGoG3eL2YMP9fCYiDkTEyYj4VNJP1OB7V0wz/rykn0fEC8Xixt+7yfrq1fvWRNi3Slpg+6u2p0taKml9A318ge0ZxQcnsj1D0rfVf1NRr5e0vLi/XNJLDfbyOf0yjXeracbV8HvX+PTnEdHzP0l3aPwT+f+V9M9N9NCir69J+p/ib2fTvUl6VuO7dR9rfI/ofkl/KWmTpLeL21l91Nu/a3xq7zc1Hqw5DfX2DY0fGr4paXvxd0fT711JXz153/i6LJAE36ADkiDsQBKEHUiCsANJEHYgCcIOJEHYgST+H2i2IiE+NC8ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] ⇒ [4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] ⇒ [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [0. 0. 0. 0. 1. 0. 0. 1. 0. 0.] ⇒ [4 7]\n"
     ]
    }
   ],
   "source": [
    "rand0 = np.random.randint(0,len(x_train))\n",
    "rand1 = np.random.randint(0,len(x_train))\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(x_train[rand0].reshape((28,28)))\n",
    "plt.show()\n",
    "print(\"Label\",t_train[rand0],\"⇒\",np.sort(t_train[rand0].argsort()[-1:]))\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(x_train[rand1].reshape((28,28)))\n",
    "plt.show()\n",
    "print(\"Label:\",t_train[rand1],\"⇒\",np.sort(t_train[rand1].argsort()[-1:]))\n",
    "\n",
    "x_train_overlap = np.maximum(x_train[rand0],x_train[rand1])\n",
    "t_train_overlap = np.maximum(t_train[rand0],t_train[rand1])\n",
    "plt.gray()\n",
    "plt.imshow(x_train_overlap.reshape((28,28)))\n",
    "plt.show()\n",
    "print(\"Label:\",t_train_overlap,\"⇒\",np.sort(t_train_overlap.argsort()[-2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にネットワークを作製したが、片方の数字しか正解しないネットワークになってしまった。  \n",
    "必死に考えたがよくわからなかったので、中間面談を利用して相談してみた。  \n",
    "  \n",
    "すると、ソフトマックス関数が1つの解答を得るときに使う関数だと教えてもらえた。  \n",
    "回帰問題になるのでDay1のプログラムを改良したほうが良いともアドバイスをもらった。  \n",
    "正直CNNになると思っていたので、これは大変ありがたいお答えでした(^^♪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(v):\n",
    "    x = np.maximum(0,v)\n",
    "    return x\n",
    "\n",
    "def MSE(t, y):\n",
    "    mse = np.mean(np.square(t-y))\n",
    "    return mse\n",
    "\n",
    "class MLP_regressor():\n",
    "    def __init__(self):\n",
    "        # 重みの定義\n",
    "        self.w1 = np.random.randn(784, 50) * 0.1\n",
    "        self.w2 = np.random.randn(50, 50) * 0.1\n",
    "        self.w3 = np.random.randn(50, 10) * 0.1\n",
    "        self.w4 = np.random.randn(10, 10) * 0.1\n",
    "\n",
    "        # バイアスの定義\n",
    "        self.b1 = np.zeros(50, dtype=float)\n",
    "        self.b2 = np.zeros(50, dtype=float)\n",
    "        self.b3 = np.zeros(10, dtype=float)\n",
    "        self.b4 = np.zeros(10, dtype=float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.layer0 = x\n",
    "        self.layer1 = relu(np.dot(self.layer0,self.w1)+self.b1)#### 問1-3 ####\n",
    "        self.layer2 = relu(np.dot(self.layer1,self.w2)+self.b2)#### 問1-4 ####\n",
    "        self.layer3 = relu(np.dot(self.layer2,self.w3)+self.b3)#### 問1-5 ####\n",
    "        self.out = np.dot(self.layer3,self.w4)+self.b4#### 問1-6 ####\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, t, y):\n",
    "        # 出力層の誤差デルタは二乗誤差の微分\n",
    "        delta4 = -2*(t-y)#### 問1-7 ####\n",
    "        # 誤差逆伝播\n",
    "        delta3 = np.dot(delta4,np.transpose(self.w4))#### 問1-8 ####\n",
    "        delta2 = np.dot(delta3*(self.layer3 > 0),np.transpose(self.w3))#### 問1-9 ####\n",
    "        delta1 = np.dot(delta2*(self.layer2 > 0),np.transpose(self.w2))#### 問1-10 ####\n",
    "\n",
    "        # バイアスbのコスト関数eに対する勾配\n",
    "        self.dedb4 = np.mean(delta4, axis=0)\n",
    "        self.dedb3 = np.mean(delta3 * (self.layer3 > 0), axis=0)\n",
    "        self.dedb2 = np.mean(delta2 * (self.layer2 > 0), axis=0)\n",
    "        self.dedb1 = np.mean(delta1 * (self.layer1 > 0), axis=0)\n",
    "\n",
    "        # 重みwのコスト関数eに対する勾配\n",
    "        self.dedw4 = np.dot(self.layer3.T, delta4) / delta4.shape[0]\n",
    "        self.dedw3 = np.dot(self.layer2.T, delta3 * (self.layer3 > 0)) / delta3.shape[0]\n",
    "        self.dedw2 = np.dot(self.layer1.T, delta2 * (self.layer2 > 0)) / delta2.shape[0]\n",
    "        self.dedw1 = np.dot(self.layer0.T, delta1 * (self.layer1 > 0)) / delta1.shape[0]\n",
    "\n",
    "    def optimize_GradientDecent(self, lr):\n",
    "        self.b1 -= lr * self.dedb1\n",
    "        self.b2 -= lr * self.dedb2\n",
    "        self.b3 -= lr * self.dedb3\n",
    "        self.b4 -= lr * self.dedb4\n",
    "\n",
    "        self.w1 -= lr * self.dedw1\n",
    "        self.w2 -= lr * self.dedw2\n",
    "        self.w3 -= lr * self.dedw3\n",
    "        self.w4 -= lr * self.dedw4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは通常のmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 TrainLoss: 0.10335845694160065 Accuracy: 0.09728571428571428\n",
      "EPOCH: 2 TrainLoss: 0.09687345934950982 Accuracy: 0.1055\n",
      "EPOCH: 3 TrainLoss: 0.09378391172193262 Accuracy: 0.11714285714285715\n",
      "EPOCH: 4 TrainLoss: 0.09201989106912394 Accuracy: 0.12964285714285714\n",
      "EPOCH: 5 TrainLoss: 0.09093290590232246 Accuracy: 0.14464285714285716\n",
      "EPOCH: 6 TrainLoss: 0.09022794242913282 Accuracy: 0.155\n",
      "EPOCH: 7 TrainLoss: 0.08974891297040091 Accuracy: 0.16592857142857143\n",
      "EPOCH: 8 TrainLoss: 0.08940814626735039 Accuracy: 0.17714285714285713\n",
      "EPOCH: 9 TrainLoss: 0.08915202581640146 Accuracy: 0.18692857142857142\n",
      "EPOCH: 10 TrainLoss: 0.08894787167000695 Accuracy: 0.1955\n",
      "EPOCH: 11 TrainLoss: 0.08877511543973528 Accuracy: 0.20135714285714285\n",
      "EPOCH: 12 TrainLoss: 0.08862156710776446 Accuracy: 0.207\n",
      "EPOCH: 13 TrainLoss: 0.0884785802186355 Accuracy: 0.21235714285714286\n",
      "EPOCH: 14 TrainLoss: 0.08834141177544573 Accuracy: 0.21828571428571428\n",
      "EPOCH: 15 TrainLoss: 0.08820664841981217 Accuracy: 0.22335714285714287\n",
      "EPOCH: 16 TrainLoss: 0.08807231466412577 Accuracy: 0.22764285714285715\n",
      "EPOCH: 17 TrainLoss: 0.087936963477697 Accuracy: 0.23192857142857143\n",
      "EPOCH: 18 TrainLoss: 0.08779986026882572 Accuracy: 0.23614285714285715\n",
      "EPOCH: 19 TrainLoss: 0.08766010388434428 Accuracy: 0.2402142857142857\n",
      "EPOCH: 20 TrainLoss: 0.08751722186059607 Accuracy: 0.24414285714285713\n",
      "EPOCH: 21 TrainLoss: 0.08737076855886328 Accuracy: 0.24692857142857144\n",
      "EPOCH: 22 TrainLoss: 0.08722059543758391 Accuracy: 0.24964285714285714\n",
      "EPOCH: 23 TrainLoss: 0.08706667667026842 Accuracy: 0.2532857142857143\n",
      "EPOCH: 24 TrainLoss: 0.0869085242800106 Accuracy: 0.2581428571428571\n",
      "EPOCH: 25 TrainLoss: 0.08674629232041145 Accuracy: 0.26235714285714284\n",
      "EPOCH: 26 TrainLoss: 0.08658005128667003 Accuracy: 0.2655\n",
      "EPOCH: 27 TrainLoss: 0.08640967564046352 Accuracy: 0.2675\n",
      "EPOCH: 28 TrainLoss: 0.08623542380466702 Accuracy: 0.26971428571428574\n",
      "EPOCH: 29 TrainLoss: 0.08605731889751687 Accuracy: 0.2725\n",
      "EPOCH: 30 TrainLoss: 0.08587541402317725 Accuracy: 0.27514285714285713\n",
      "EPOCH: 31 TrainLoss: 0.08568992106895978 Accuracy: 0.2779285714285714\n",
      "EPOCH: 32 TrainLoss: 0.08550090876971854 Accuracy: 0.2815\n",
      "EPOCH: 33 TrainLoss: 0.08530841261460681 Accuracy: 0.2855\n",
      "EPOCH: 34 TrainLoss: 0.085112654409861 Accuracy: 0.28914285714285715\n",
      "EPOCH: 35 TrainLoss: 0.08491375411680516 Accuracy: 0.29235714285714287\n",
      "EPOCH: 36 TrainLoss: 0.08471158474040698 Accuracy: 0.2945714285714286\n",
      "EPOCH: 37 TrainLoss: 0.08450631499526859 Accuracy: 0.2973571428571429\n",
      "EPOCH: 38 TrainLoss: 0.0842981805193786 Accuracy: 0.3003571428571429\n",
      "EPOCH: 39 TrainLoss: 0.08408720927360756 Accuracy: 0.30378571428571427\n",
      "EPOCH: 40 TrainLoss: 0.08387337287881694 Accuracy: 0.3067142857142857\n",
      "EPOCH: 41 TrainLoss: 0.08365691301275661 Accuracy: 0.31\n",
      "EPOCH: 42 TrainLoss: 0.08343786882748908 Accuracy: 0.3132142857142857\n",
      "EPOCH: 43 TrainLoss: 0.08321639546496808 Accuracy: 0.31642857142857145\n",
      "EPOCH: 44 TrainLoss: 0.08299225211827495 Accuracy: 0.31992857142857145\n",
      "EPOCH: 45 TrainLoss: 0.08276534127557952 Accuracy: 0.32292857142857145\n",
      "EPOCH: 46 TrainLoss: 0.08253543710156305 Accuracy: 0.3259285714285714\n",
      "EPOCH: 47 TrainLoss: 0.0823027230822831 Accuracy: 0.3297857142857143\n",
      "EPOCH: 48 TrainLoss: 0.08206737016942822 Accuracy: 0.3335\n",
      "EPOCH: 49 TrainLoss: 0.08182945955754296 Accuracy: 0.3372142857142857\n",
      "EPOCH: 50 TrainLoss: 0.0815889436044944 Accuracy: 0.342\n",
      "EPOCH: 51 TrainLoss: 0.08134595701821372 Accuracy: 0.34664285714285714\n",
      "EPOCH: 52 TrainLoss: 0.08110062998575962 Accuracy: 0.3500714285714286\n",
      "EPOCH: 53 TrainLoss: 0.08085314116977294 Accuracy: 0.3540714285714286\n",
      "EPOCH: 54 TrainLoss: 0.08060299114269114 Accuracy: 0.35792857142857143\n",
      "EPOCH: 55 TrainLoss: 0.08035036208388378 Accuracy: 0.362\n",
      "EPOCH: 56 TrainLoss: 0.08009565931617918 Accuracy: 0.36492857142857144\n",
      "EPOCH: 57 TrainLoss: 0.07983890892049943 Accuracy: 0.36828571428571427\n",
      "EPOCH: 58 TrainLoss: 0.07958005693366194 Accuracy: 0.3717142857142857\n",
      "EPOCH: 59 TrainLoss: 0.07931955590968368 Accuracy: 0.375\n",
      "EPOCH: 60 TrainLoss: 0.07905764192821327 Accuracy: 0.37807142857142856\n",
      "EPOCH: 61 TrainLoss: 0.07879475432648239 Accuracy: 0.3812142857142857\n",
      "EPOCH: 62 TrainLoss: 0.07853072281222971 Accuracy: 0.38492857142857145\n",
      "EPOCH: 63 TrainLoss: 0.07826523731271094 Accuracy: 0.38821428571428573\n",
      "EPOCH: 64 TrainLoss: 0.0779988439375986 Accuracy: 0.3917857142857143\n",
      "EPOCH: 65 TrainLoss: 0.07773193972134035 Accuracy: 0.3939285714285714\n",
      "EPOCH: 66 TrainLoss: 0.07746541153532568 Accuracy: 0.397\n",
      "EPOCH: 67 TrainLoss: 0.07719917465738689 Accuracy: 0.4012142857142857\n",
      "EPOCH: 68 TrainLoss: 0.07693309653203866 Accuracy: 0.4039285714285714\n",
      "EPOCH: 69 TrainLoss: 0.07666743573465844 Accuracy: 0.4075\n",
      "EPOCH: 70 TrainLoss: 0.07640240492979007 Accuracy: 0.4104285714285714\n",
      "EPOCH: 71 TrainLoss: 0.07613835393666987 Accuracy: 0.4135714285714286\n",
      "EPOCH: 72 TrainLoss: 0.07587562989963609 Accuracy: 0.41635714285714287\n",
      "EPOCH: 73 TrainLoss: 0.07561427460284241 Accuracy: 0.41914285714285715\n",
      "EPOCH: 74 TrainLoss: 0.0753543638517571 Accuracy: 0.4228571428571429\n",
      "EPOCH: 75 TrainLoss: 0.07509582882914746 Accuracy: 0.42642857142857143\n",
      "EPOCH: 76 TrainLoss: 0.07483898205490189 Accuracy: 0.42928571428571427\n",
      "EPOCH: 77 TrainLoss: 0.07458423616565105 Accuracy: 0.43242857142857144\n",
      "EPOCH: 78 TrainLoss: 0.07433180141740081 Accuracy: 0.43414285714285716\n",
      "EPOCH: 79 TrainLoss: 0.07408188713262631 Accuracy: 0.43857142857142856\n",
      "EPOCH: 80 TrainLoss: 0.07383446126948627 Accuracy: 0.44157142857142856\n",
      "EPOCH: 81 TrainLoss: 0.0735895465367643 Accuracy: 0.44421428571428573\n",
      "EPOCH: 82 TrainLoss: 0.07334715129360984 Accuracy: 0.44785714285714284\n",
      "EPOCH: 83 TrainLoss: 0.07310729969635829 Accuracy: 0.45071428571428573\n",
      "EPOCH: 84 TrainLoss: 0.0728700504698083 Accuracy: 0.4527857142857143\n",
      "EPOCH: 85 TrainLoss: 0.07263554355319905 Accuracy: 0.45535714285714285\n",
      "EPOCH: 86 TrainLoss: 0.07240348060480009 Accuracy: 0.457\n",
      "EPOCH: 87 TrainLoss: 0.07217388339717451 Accuracy: 0.4590714285714286\n",
      "EPOCH: 88 TrainLoss: 0.07194658889036111 Accuracy: 0.4614285714285714\n",
      "EPOCH: 89 TrainLoss: 0.07172170738013951 Accuracy: 0.4637142857142857\n",
      "EPOCH: 90 TrainLoss: 0.07149896119469595 Accuracy: 0.46614285714285714\n",
      "EPOCH: 91 TrainLoss: 0.07127823556755497 Accuracy: 0.4685714285714286\n",
      "EPOCH: 92 TrainLoss: 0.07105915323440914 Accuracy: 0.4715\n",
      "EPOCH: 93 TrainLoss: 0.070841727578825 Accuracy: 0.47385714285714287\n",
      "EPOCH: 94 TrainLoss: 0.07062576882664888 Accuracy: 0.4767142857142857\n",
      "EPOCH: 95 TrainLoss: 0.0704109516612483 Accuracy: 0.47914285714285715\n",
      "EPOCH: 96 TrainLoss: 0.07019717849200999 Accuracy: 0.4823571428571429\n",
      "EPOCH: 97 TrainLoss: 0.06998432975471737 Accuracy: 0.486\n",
      "EPOCH: 98 TrainLoss: 0.06977229945337976 Accuracy: 0.4878571428571429\n",
      "EPOCH: 99 TrainLoss: 0.06956088818937906 Accuracy: 0.4903571428571429\n",
      "EPOCH: 100 TrainLoss: 0.06934974805492036 Accuracy: 0.4935\n",
      "EPOCH: 101 TrainLoss: 0.0691384175118695 Accuracy: 0.4952857142857143\n",
      "EPOCH: 102 TrainLoss: 0.0689268419287651 Accuracy: 0.49785714285714283\n",
      "EPOCH: 103 TrainLoss: 0.06871534059638648 Accuracy: 0.49985714285714283\n",
      "EPOCH: 104 TrainLoss: 0.06850373452614064 Accuracy: 0.5023571428571428\n",
      "EPOCH: 105 TrainLoss: 0.06829185237479861 Accuracy: 0.5050714285714286\n",
      "EPOCH: 106 TrainLoss: 0.06807967460451483 Accuracy: 0.5073571428571428\n",
      "EPOCH: 107 TrainLoss: 0.06786742593645459 Accuracy: 0.5091428571428571\n",
      "EPOCH: 108 TrainLoss: 0.06765478726724022 Accuracy: 0.5111428571428571\n",
      "EPOCH: 109 TrainLoss: 0.06744179749635146 Accuracy: 0.514\n",
      "EPOCH: 110 TrainLoss: 0.06722835130852654 Accuracy: 0.5165\n",
      "EPOCH: 111 TrainLoss: 0.06701442033561601 Accuracy: 0.5187142857142857\n",
      "EPOCH: 112 TrainLoss: 0.06680011204412604 Accuracy: 0.5204285714285715\n",
      "EPOCH: 113 TrainLoss: 0.06658532610233406 Accuracy: 0.5228571428571429\n",
      "EPOCH: 114 TrainLoss: 0.06637022033674357 Accuracy: 0.5245\n",
      "EPOCH: 115 TrainLoss: 0.06615491113859631 Accuracy: 0.5266428571428572\n",
      "EPOCH: 116 TrainLoss: 0.06593950789482245 Accuracy: 0.5286428571428572\n",
      "EPOCH: 117 TrainLoss: 0.06572426549693798 Accuracy: 0.5307142857142857\n",
      "EPOCH: 118 TrainLoss: 0.06550937045510709 Accuracy: 0.5326428571428572\n",
      "EPOCH: 119 TrainLoss: 0.06529495739068197 Accuracy: 0.5347142857142857\n",
      "EPOCH: 120 TrainLoss: 0.06508116147915927 Accuracy: 0.5365714285714286\n",
      "EPOCH: 121 TrainLoss: 0.0648681507736341 Accuracy: 0.5397142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 122 TrainLoss: 0.06465635653599325 Accuracy: 0.5415\n",
      "EPOCH: 123 TrainLoss: 0.06444584679860223 Accuracy: 0.5432857142857143\n",
      "EPOCH: 124 TrainLoss: 0.06423689799437593 Accuracy: 0.5457142857142857\n",
      "EPOCH: 125 TrainLoss: 0.06402980375210673 Accuracy: 0.5477857142857143\n",
      "EPOCH: 126 TrainLoss: 0.06382458332934937 Accuracy: 0.5497142857142857\n",
      "EPOCH: 127 TrainLoss: 0.0636217234672083 Accuracy: 0.5519285714285714\n",
      "EPOCH: 128 TrainLoss: 0.06342096426517352 Accuracy: 0.5537857142857143\n",
      "EPOCH: 129 TrainLoss: 0.06322257277454077 Accuracy: 0.5557142857142857\n",
      "EPOCH: 130 TrainLoss: 0.06302644268815558 Accuracy: 0.5570714285714286\n",
      "EPOCH: 131 TrainLoss: 0.06283276778641725 Accuracy: 0.5602857142857143\n",
      "EPOCH: 132 TrainLoss: 0.06264168173404697 Accuracy: 0.5627857142857143\n",
      "EPOCH: 133 TrainLoss: 0.0624533029383825 Accuracy: 0.5649285714285714\n",
      "EPOCH: 134 TrainLoss: 0.0622674106740756 Accuracy: 0.5682142857142857\n",
      "EPOCH: 135 TrainLoss: 0.0620841752814347 Accuracy: 0.5695\n",
      "EPOCH: 136 TrainLoss: 0.0619036141025217 Accuracy: 0.5712142857142857\n",
      "EPOCH: 137 TrainLoss: 0.06172568032807899 Accuracy: 0.5726428571428571\n",
      "EPOCH: 138 TrainLoss: 0.06155029989759969 Accuracy: 0.5744285714285714\n",
      "EPOCH: 139 TrainLoss: 0.061377207597702856 Accuracy: 0.5757857142857142\n",
      "EPOCH: 140 TrainLoss: 0.061206580108245805 Accuracy: 0.577\n",
      "EPOCH: 141 TrainLoss: 0.06103839810283164 Accuracy: 0.579\n",
      "EPOCH: 142 TrainLoss: 0.06087242361274367 Accuracy: 0.5809285714285715\n",
      "EPOCH: 143 TrainLoss: 0.0607084881451383 Accuracy: 0.5833571428571429\n",
      "EPOCH: 144 TrainLoss: 0.060546441424518226 Accuracy: 0.5847857142857142\n",
      "EPOCH: 145 TrainLoss: 0.06038624865961245 Accuracy: 0.5865714285714285\n",
      "EPOCH: 146 TrainLoss: 0.06022787878108628 Accuracy: 0.5880714285714286\n",
      "EPOCH: 147 TrainLoss: 0.06007142178061541 Accuracy: 0.5895\n",
      "EPOCH: 148 TrainLoss: 0.05991664651875791 Accuracy: 0.5915714285714285\n",
      "EPOCH: 149 TrainLoss: 0.0597634013866509 Accuracy: 0.5931428571428572\n",
      "EPOCH: 150 TrainLoss: 0.0596118078159324 Accuracy: 0.5949285714285715\n",
      "EPOCH: 151 TrainLoss: 0.05946176382225913 Accuracy: 0.5972142857142857\n",
      "EPOCH: 152 TrainLoss: 0.059313218898507755 Accuracy: 0.5988571428571429\n",
      "EPOCH: 153 TrainLoss: 0.059166007350431836 Accuracy: 0.6003571428571428\n",
      "EPOCH: 154 TrainLoss: 0.0590201211304755 Accuracy: 0.6010714285714286\n",
      "EPOCH: 155 TrainLoss: 0.05887536666935178 Accuracy: 0.602\n",
      "EPOCH: 156 TrainLoss: 0.05873184653088583 Accuracy: 0.6033571428571428\n",
      "EPOCH: 157 TrainLoss: 0.05858957740150685 Accuracy: 0.6044285714285714\n",
      "EPOCH: 158 TrainLoss: 0.05844858709185651 Accuracy: 0.6056428571428571\n",
      "EPOCH: 159 TrainLoss: 0.05830874061868178 Accuracy: 0.6069285714285715\n",
      "EPOCH: 160 TrainLoss: 0.05816988789619147 Accuracy: 0.6075\n",
      "EPOCH: 161 TrainLoss: 0.05803197778134848 Accuracy: 0.609\n",
      "EPOCH: 162 TrainLoss: 0.057894940605721784 Accuracy: 0.6106428571428572\n",
      "EPOCH: 163 TrainLoss: 0.05775883930111667 Accuracy: 0.6120714285714286\n",
      "EPOCH: 164 TrainLoss: 0.057623645938145215 Accuracy: 0.6135714285714285\n",
      "EPOCH: 165 TrainLoss: 0.05748942232163035 Accuracy: 0.6147857142857143\n",
      "EPOCH: 166 TrainLoss: 0.05735583378646271 Accuracy: 0.6155\n",
      "EPOCH: 167 TrainLoss: 0.057223007986238916 Accuracy: 0.6166428571428572\n",
      "EPOCH: 168 TrainLoss: 0.05709088897962323 Accuracy: 0.618\n",
      "EPOCH: 169 TrainLoss: 0.056959464203482836 Accuracy: 0.6187142857142857\n",
      "EPOCH: 170 TrainLoss: 0.056828743281727274 Accuracy: 0.6196428571428572\n",
      "EPOCH: 171 TrainLoss: 0.0566987039503795 Accuracy: 0.6207142857142857\n",
      "EPOCH: 172 TrainLoss: 0.056569132898241385 Accuracy: 0.622\n",
      "EPOCH: 173 TrainLoss: 0.056440047765499386 Accuracy: 0.6229285714285714\n",
      "EPOCH: 174 TrainLoss: 0.05631145758418502 Accuracy: 0.6232142857142857\n",
      "EPOCH: 175 TrainLoss: 0.05618321878521983 Accuracy: 0.6243571428571428\n",
      "EPOCH: 176 TrainLoss: 0.05605533609945501 Accuracy: 0.6258571428571429\n",
      "EPOCH: 177 TrainLoss: 0.055927747921127896 Accuracy: 0.6267142857142857\n",
      "EPOCH: 178 TrainLoss: 0.05580044627116319 Accuracy: 0.6278571428571429\n",
      "EPOCH: 179 TrainLoss: 0.05567340933955878 Accuracy: 0.6289285714285714\n",
      "EPOCH: 180 TrainLoss: 0.055546767494979526 Accuracy: 0.6299285714285714\n",
      "EPOCH: 181 TrainLoss: 0.05542013775623084 Accuracy: 0.6312142857142857\n",
      "EPOCH: 182 TrainLoss: 0.055293944787015416 Accuracy: 0.6318571428571429\n",
      "EPOCH: 183 TrainLoss: 0.05516807911365906 Accuracy: 0.633\n",
      "EPOCH: 184 TrainLoss: 0.05504245236301812 Accuracy: 0.6349285714285714\n",
      "EPOCH: 185 TrainLoss: 0.05491700375131769 Accuracy: 0.6360714285714286\n",
      "EPOCH: 186 TrainLoss: 0.054791811636068666 Accuracy: 0.6372857142857142\n",
      "EPOCH: 187 TrainLoss: 0.05466659097680771 Accuracy: 0.6385714285714286\n",
      "EPOCH: 188 TrainLoss: 0.05454127749095029 Accuracy: 0.64\n",
      "EPOCH: 189 TrainLoss: 0.054416152221202295 Accuracy: 0.6412142857142857\n",
      "EPOCH: 190 TrainLoss: 0.05429118582069493 Accuracy: 0.6427857142857143\n",
      "EPOCH: 191 TrainLoss: 0.054166427547325056 Accuracy: 0.6436428571428572\n",
      "EPOCH: 192 TrainLoss: 0.05404171499373299 Accuracy: 0.6449285714285714\n",
      "EPOCH: 193 TrainLoss: 0.05391716130767139 Accuracy: 0.6459285714285714\n",
      "EPOCH: 194 TrainLoss: 0.05379272365713164 Accuracy: 0.6468571428571429\n",
      "EPOCH: 195 TrainLoss: 0.05366840824232181 Accuracy: 0.6477142857142857\n",
      "EPOCH: 196 TrainLoss: 0.05354422802101063 Accuracy: 0.649\n",
      "EPOCH: 197 TrainLoss: 0.05342013546166329 Accuracy: 0.6503571428571429\n",
      "EPOCH: 198 TrainLoss: 0.05329614631167366 Accuracy: 0.6522142857142857\n",
      "EPOCH: 199 TrainLoss: 0.053172300629060275 Accuracy: 0.6535\n",
      "EPOCH: 200 TrainLoss: 0.05304854057627928 Accuracy: 0.6546428571428572\n",
      "EPOCH: 201 TrainLoss: 0.052924825350182364 Accuracy: 0.6554285714285715\n",
      "EPOCH: 202 TrainLoss: 0.052801284576251474 Accuracy: 0.6563571428571429\n",
      "EPOCH: 203 TrainLoss: 0.052677819549366046 Accuracy: 0.6576428571428572\n",
      "EPOCH: 204 TrainLoss: 0.05255447128035522 Accuracy: 0.6590714285714285\n",
      "EPOCH: 205 TrainLoss: 0.05243111289644689 Accuracy: 0.6602857142857143\n",
      "EPOCH: 206 TrainLoss: 0.052307761931312445 Accuracy: 0.6614285714285715\n",
      "EPOCH: 207 TrainLoss: 0.052184505040817765 Accuracy: 0.6627857142857143\n",
      "EPOCH: 208 TrainLoss: 0.052061274960585 Accuracy: 0.6637857142857143\n",
      "EPOCH: 209 TrainLoss: 0.05193814730660973 Accuracy: 0.6647857142857143\n",
      "EPOCH: 210 TrainLoss: 0.05181512457854138 Accuracy: 0.6656428571428571\n",
      "EPOCH: 211 TrainLoss: 0.05169214126577362 Accuracy: 0.6665714285714286\n",
      "EPOCH: 212 TrainLoss: 0.051569154257694386 Accuracy: 0.6671428571428571\n",
      "EPOCH: 213 TrainLoss: 0.05144624613091798 Accuracy: 0.6685\n",
      "EPOCH: 214 TrainLoss: 0.051323404836699 Accuracy: 0.6698571428571428\n",
      "EPOCH: 215 TrainLoss: 0.051200653063467105 Accuracy: 0.6715\n",
      "EPOCH: 216 TrainLoss: 0.051078005844789214 Accuracy: 0.673\n",
      "EPOCH: 217 TrainLoss: 0.050955526154880464 Accuracy: 0.6742857142857143\n",
      "EPOCH: 218 TrainLoss: 0.05083310348621355 Accuracy: 0.6759285714285714\n",
      "EPOCH: 219 TrainLoss: 0.0507107977381248 Accuracy: 0.6767857142857143\n",
      "EPOCH: 220 TrainLoss: 0.05058863468779604 Accuracy: 0.6785714285714286\n",
      "EPOCH: 221 TrainLoss: 0.05046656893237041 Accuracy: 0.6797857142857143\n",
      "EPOCH: 222 TrainLoss: 0.05034486782941481 Accuracy: 0.6806428571428571\n",
      "EPOCH: 223 TrainLoss: 0.050223610539640494 Accuracy: 0.6811428571428572\n",
      "EPOCH: 224 TrainLoss: 0.05010321723851119 Accuracy: 0.6825\n",
      "EPOCH: 225 TrainLoss: 0.0499837595754567 Accuracy: 0.6839285714285714\n",
      "EPOCH: 226 TrainLoss: 0.04986635519347099 Accuracy: 0.685\n",
      "EPOCH: 227 TrainLoss: 0.049751080490335844 Accuracy: 0.6859285714285714\n",
      "EPOCH: 228 TrainLoss: 0.049640226699147534 Accuracy: 0.6869285714285714\n",
      "EPOCH: 229 TrainLoss: 0.04953461225723047 Accuracy: 0.6893571428571429\n",
      "EPOCH: 230 TrainLoss: 0.04943919034038543 Accuracy: 0.6877142857142857\n",
      "EPOCH: 231 TrainLoss: 0.049356640792688825 Accuracy: 0.6907142857142857\n",
      "EPOCH: 232 TrainLoss: 0.049300449296423514 Accuracy: 0.6888571428571428\n",
      "EPOCH: 233 TrainLoss: 0.049273968538021407 Accuracy: 0.6923571428571429\n",
      "EPOCH: 234 TrainLoss: 0.04930730582602422 Accuracy: 0.6905\n",
      "EPOCH: 235 TrainLoss: 0.049388101148062025 Accuracy: 0.6930714285714286\n",
      "EPOCH: 236 TrainLoss: 0.04959932754819804 Accuracy: 0.6896428571428571\n",
      "EPOCH: 237 TrainLoss: 0.04980337799405649 Accuracy: 0.6915\n",
      "EPOCH: 238 TrainLoss: 0.05019214294988329 Accuracy: 0.6890714285714286\n",
      "EPOCH: 239 TrainLoss: 0.05026735809789687 Accuracy: 0.6895\n",
      "EPOCH: 240 TrainLoss: 0.05050775226023756 Accuracy: 0.6912857142857143\n",
      "EPOCH: 241 TrainLoss: 0.0500891944505457 Accuracy: 0.6922142857142857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 242 TrainLoss: 0.0498982683633681 Accuracy: 0.6954285714285714\n",
      "EPOCH: 243 TrainLoss: 0.04926526971979799 Accuracy: 0.6958571428571428\n",
      "EPOCH: 244 TrainLoss: 0.0489211249511457 Accuracy: 0.6992857142857143\n",
      "EPOCH: 245 TrainLoss: 0.04844794927060087 Accuracy: 0.7002857142857143\n",
      "EPOCH: 246 TrainLoss: 0.048165186427586194 Accuracy: 0.7015\n",
      "EPOCH: 247 TrainLoss: 0.04785634513666745 Accuracy: 0.703\n",
      "EPOCH: 248 TrainLoss: 0.04764280686010356 Accuracy: 0.7029285714285715\n",
      "EPOCH: 249 TrainLoss: 0.047423482254694026 Accuracy: 0.7055714285714285\n",
      "EPOCH: 250 TrainLoss: 0.047252765096403575 Accuracy: 0.7046428571428571\n",
      "EPOCH: 251 TrainLoss: 0.04708020183515237 Accuracy: 0.7076428571428571\n",
      "EPOCH: 252 TrainLoss: 0.04693286600525154 Accuracy: 0.706\n",
      "EPOCH: 253 TrainLoss: 0.04678129663457348 Accuracy: 0.7089285714285715\n",
      "EPOCH: 254 TrainLoss: 0.04664680480024188 Accuracy: 0.7086428571428571\n",
      "EPOCH: 255 TrainLoss: 0.046508704954759866 Accuracy: 0.7097142857142857\n",
      "EPOCH: 256 TrainLoss: 0.046382745165289875 Accuracy: 0.7096428571428571\n",
      "EPOCH: 257 TrainLoss: 0.04625215583141667 Accuracy: 0.7112142857142857\n",
      "EPOCH: 258 TrainLoss: 0.04613191624618997 Accuracy: 0.7113571428571429\n",
      "EPOCH: 259 TrainLoss: 0.04600661182308594 Accuracy: 0.7131428571428572\n",
      "EPOCH: 260 TrainLoss: 0.045890796364275906 Accuracy: 0.7129285714285715\n",
      "EPOCH: 261 TrainLoss: 0.04576923355784892 Accuracy: 0.7149285714285715\n",
      "EPOCH: 262 TrainLoss: 0.045657569860530114 Accuracy: 0.7138571428571429\n",
      "EPOCH: 263 TrainLoss: 0.04553869299625167 Accuracy: 0.7166428571428571\n",
      "EPOCH: 264 TrainLoss: 0.045431394832188776 Accuracy: 0.7155714285714285\n",
      "EPOCH: 265 TrainLoss: 0.04531616510908669 Accuracy: 0.7181428571428572\n",
      "EPOCH: 266 TrainLoss: 0.045214284992774904 Accuracy: 0.7164285714285714\n",
      "EPOCH: 267 TrainLoss: 0.04510297149256939 Accuracy: 0.7197857142857143\n",
      "EPOCH: 268 TrainLoss: 0.0450086027697484 Accuracy: 0.7172857142857143\n",
      "EPOCH: 269 TrainLoss: 0.044901159450805694 Accuracy: 0.7212142857142857\n",
      "EPOCH: 270 TrainLoss: 0.04481748870721602 Accuracy: 0.7181428571428572\n",
      "EPOCH: 271 TrainLoss: 0.04471285889039455 Accuracy: 0.7217142857142858\n",
      "EPOCH: 272 TrainLoss: 0.04464344395111138 Accuracy: 0.7200714285714286\n",
      "EPOCH: 273 TrainLoss: 0.04454293797493309 Accuracy: 0.7235714285714285\n",
      "EPOCH: 274 TrainLoss: 0.04449167289295087 Accuracy: 0.7209285714285715\n",
      "EPOCH: 275 TrainLoss: 0.04439210274492731 Accuracy: 0.7243571428571428\n",
      "EPOCH: 276 TrainLoss: 0.04436220878242303 Accuracy: 0.7221428571428572\n",
      "EPOCH: 277 TrainLoss: 0.04425715925362322 Accuracy: 0.7257142857142858\n",
      "EPOCH: 278 TrainLoss: 0.04425187940813978 Accuracy: 0.7227142857142858\n",
      "EPOCH: 279 TrainLoss: 0.0441275438184211 Accuracy: 0.727\n",
      "EPOCH: 280 TrainLoss: 0.04414158836508049 Accuracy: 0.7236428571428571\n",
      "EPOCH: 281 TrainLoss: 0.04399309845563199 Accuracy: 0.7275\n",
      "EPOCH: 282 TrainLoss: 0.04401954023110106 Accuracy: 0.7245\n",
      "EPOCH: 283 TrainLoss: 0.04383927325304556 Accuracy: 0.7293571428571428\n",
      "EPOCH: 284 TrainLoss: 0.04386391080730969 Accuracy: 0.7257142857142858\n",
      "EPOCH: 285 TrainLoss: 0.04365555363123828 Accuracy: 0.7305714285714285\n",
      "EPOCH: 286 TrainLoss: 0.043669400302135515 Accuracy: 0.7265714285714285\n",
      "EPOCH: 287 TrainLoss: 0.0434472289806013 Accuracy: 0.7323571428571428\n",
      "EPOCH: 288 TrainLoss: 0.04344405245328471 Accuracy: 0.7275\n",
      "EPOCH: 289 TrainLoss: 0.0432192613775103 Accuracy: 0.7334285714285714\n",
      "EPOCH: 290 TrainLoss: 0.04319944339755799 Accuracy: 0.7295\n",
      "EPOCH: 291 TrainLoss: 0.04298413811320778 Accuracy: 0.735\n",
      "EPOCH: 292 TrainLoss: 0.04295157879765118 Accuracy: 0.7316428571428572\n",
      "EPOCH: 293 TrainLoss: 0.042753328781048996 Accuracy: 0.7357142857142858\n",
      "EPOCH: 294 TrainLoss: 0.042713680979634745 Accuracy: 0.7327142857142858\n",
      "EPOCH: 295 TrainLoss: 0.04253090347337338 Accuracy: 0.7372857142857143\n",
      "EPOCH: 296 TrainLoss: 0.042485454718607686 Accuracy: 0.735\n",
      "EPOCH: 297 TrainLoss: 0.04231847727323249 Accuracy: 0.7378571428571429\n",
      "EPOCH: 298 TrainLoss: 0.04227079201841938 Accuracy: 0.7360714285714286\n",
      "EPOCH: 299 TrainLoss: 0.042117063983261385 Accuracy: 0.7386428571428572\n",
      "EPOCH: 300 TrainLoss: 0.042069857690001794 Accuracy: 0.7374285714285714\n",
      "EPOCH: 301 TrainLoss: 0.04192729186240027 Accuracy: 0.7402857142857143\n",
      "EPOCH: 302 TrainLoss: 0.0418797598625785 Accuracy: 0.7391428571428571\n",
      "EPOCH: 303 TrainLoss: 0.04174558157495097 Accuracy: 0.7410714285714286\n",
      "EPOCH: 304 TrainLoss: 0.041698860930871384 Accuracy: 0.7405\n",
      "EPOCH: 305 TrainLoss: 0.04157227000276205 Accuracy: 0.7417857142857143\n",
      "EPOCH: 306 TrainLoss: 0.041529203678330226 Accuracy: 0.7419285714285714\n",
      "EPOCH: 307 TrainLoss: 0.04140751604986954 Accuracy: 0.7438571428571429\n",
      "EPOCH: 308 TrainLoss: 0.041368083938324224 Accuracy: 0.7432142857142857\n",
      "EPOCH: 309 TrainLoss: 0.04124966531876641 Accuracy: 0.7455\n",
      "EPOCH: 310 TrainLoss: 0.041212668629546756 Accuracy: 0.744\n",
      "EPOCH: 311 TrainLoss: 0.041096353795290856 Accuracy: 0.7477142857142857\n",
      "EPOCH: 312 TrainLoss: 0.04106268426071467 Accuracy: 0.7457857142857143\n",
      "EPOCH: 313 TrainLoss: 0.04094874683853322 Accuracy: 0.7489285714285714\n",
      "EPOCH: 314 TrainLoss: 0.040918202447055706 Accuracy: 0.7473571428571428\n",
      "EPOCH: 315 TrainLoss: 0.04080569379383454 Accuracy: 0.7497857142857143\n",
      "EPOCH: 316 TrainLoss: 0.040779669816387865 Accuracy: 0.7491428571428571\n",
      "EPOCH: 317 TrainLoss: 0.04066771807784701 Accuracy: 0.7513571428571428\n",
      "EPOCH: 318 TrainLoss: 0.04064532162427591 Accuracy: 0.7504285714285714\n",
      "EPOCH: 319 TrainLoss: 0.040532631717717454 Accuracy: 0.7522142857142857\n",
      "EPOCH: 320 TrainLoss: 0.04051349609755557 Accuracy: 0.7513571428571428\n",
      "EPOCH: 321 TrainLoss: 0.04040148537175514 Accuracy: 0.7537857142857143\n",
      "EPOCH: 322 TrainLoss: 0.040385563878102305 Accuracy: 0.7526428571428572\n",
      "EPOCH: 323 TrainLoss: 0.040275480888875684 Accuracy: 0.7551428571428571\n",
      "EPOCH: 324 TrainLoss: 0.04026369913900309 Accuracy: 0.7536428571428572\n",
      "EPOCH: 325 TrainLoss: 0.04015220950238196 Accuracy: 0.7564285714285715\n",
      "EPOCH: 326 TrainLoss: 0.040144570984936186 Accuracy: 0.7547857142857143\n",
      "EPOCH: 327 TrainLoss: 0.040031428976617225 Accuracy: 0.758\n",
      "EPOCH: 328 TrainLoss: 0.04002472048634441 Accuracy: 0.756\n",
      "EPOCH: 329 TrainLoss: 0.039908719874759774 Accuracy: 0.7592857142857142\n",
      "EPOCH: 330 TrainLoss: 0.039904396057433636 Accuracy: 0.7572142857142857\n",
      "EPOCH: 331 TrainLoss: 0.039786720772086795 Accuracy: 0.7615714285714286\n",
      "EPOCH: 332 TrainLoss: 0.03978374153398644 Accuracy: 0.7579285714285714\n",
      "EPOCH: 333 TrainLoss: 0.03966503676524461 Accuracy: 0.7629285714285714\n",
      "EPOCH: 334 TrainLoss: 0.039663734066373024 Accuracy: 0.7592857142857142\n",
      "EPOCH: 335 TrainLoss: 0.039542877696517 Accuracy: 0.7643571428571428\n",
      "EPOCH: 336 TrainLoss: 0.0395412662753385 Accuracy: 0.7602857142857142\n",
      "EPOCH: 337 TrainLoss: 0.039421266822835505 Accuracy: 0.7657142857142857\n",
      "EPOCH: 338 TrainLoss: 0.03942022703314413 Accuracy: 0.761\n",
      "EPOCH: 339 TrainLoss: 0.039300266506957074 Accuracy: 0.7672142857142857\n",
      "EPOCH: 340 TrainLoss: 0.03929587414231017 Accuracy: 0.7623571428571428\n",
      "EPOCH: 341 TrainLoss: 0.03917726612866776 Accuracy: 0.7687857142857143\n",
      "EPOCH: 342 TrainLoss: 0.0391702867738893 Accuracy: 0.7633571428571428\n",
      "EPOCH: 343 TrainLoss: 0.03905151005497321 Accuracy: 0.7710714285714285\n",
      "EPOCH: 344 TrainLoss: 0.03904400489549727 Accuracy: 0.7644285714285715\n",
      "EPOCH: 345 TrainLoss: 0.03892561982334574 Accuracy: 0.7720714285714285\n",
      "EPOCH: 346 TrainLoss: 0.0389147872635617 Accuracy: 0.7653571428571428\n",
      "EPOCH: 347 TrainLoss: 0.03879628200133253 Accuracy: 0.7728571428571429\n",
      "EPOCH: 348 TrainLoss: 0.03878199934563337 Accuracy: 0.7665\n",
      "EPOCH: 349 TrainLoss: 0.03866507435626151 Accuracy: 0.7742857142857142\n",
      "EPOCH: 350 TrainLoss: 0.03864663808174497 Accuracy: 0.7675714285714286\n",
      "EPOCH: 351 TrainLoss: 0.038532810869373484 Accuracy: 0.7760714285714285\n",
      "EPOCH: 352 TrainLoss: 0.038509113160833934 Accuracy: 0.7691428571428571\n",
      "EPOCH: 353 TrainLoss: 0.03839825357506108 Accuracy: 0.7774285714285715\n",
      "EPOCH: 354 TrainLoss: 0.038372648307546844 Accuracy: 0.7707142857142857\n",
      "EPOCH: 355 TrainLoss: 0.03826520695257419 Accuracy: 0.7796428571428572\n",
      "EPOCH: 356 TrainLoss: 0.03823918608330794 Accuracy: 0.7714285714285715\n",
      "EPOCH: 357 TrainLoss: 0.03813394546955868 Accuracy: 0.7807857142857143\n",
      "EPOCH: 358 TrainLoss: 0.03810438554824994 Accuracy: 0.7727142857142857\n",
      "EPOCH: 359 TrainLoss: 0.038001749278594114 Accuracy: 0.7815\n",
      "EPOCH: 360 TrainLoss: 0.03796923716488076 Accuracy: 0.7742857142857142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 361 TrainLoss: 0.03786971925436226 Accuracy: 0.7827142857142857\n",
      "EPOCH: 362 TrainLoss: 0.03783641217861882 Accuracy: 0.7754285714285715\n",
      "EPOCH: 363 TrainLoss: 0.037740986000096045 Accuracy: 0.7836428571428572\n",
      "EPOCH: 364 TrainLoss: 0.03770606240053916 Accuracy: 0.7772142857142857\n",
      "EPOCH: 365 TrainLoss: 0.037613469668524926 Accuracy: 0.7847857142857143\n",
      "EPOCH: 366 TrainLoss: 0.037577041135821596 Accuracy: 0.7784285714285715\n",
      "EPOCH: 367 TrainLoss: 0.03748673766899416 Accuracy: 0.786\n",
      "EPOCH: 368 TrainLoss: 0.03744899861587791 Accuracy: 0.7796428571428572\n",
      "EPOCH: 369 TrainLoss: 0.03735993658178769 Accuracy: 0.7876428571428571\n",
      "EPOCH: 370 TrainLoss: 0.03732187850421772 Accuracy: 0.7812142857142857\n",
      "EPOCH: 371 TrainLoss: 0.037236333861833615 Accuracy: 0.7887857142857143\n",
      "EPOCH: 372 TrainLoss: 0.0371983548096307 Accuracy: 0.7825714285714286\n",
      "EPOCH: 373 TrainLoss: 0.03711501445970396 Accuracy: 0.7899285714285714\n",
      "EPOCH: 374 TrainLoss: 0.037075915734956416 Accuracy: 0.7842142857142858\n",
      "EPOCH: 375 TrainLoss: 0.0369950448334648 Accuracy: 0.7907857142857143\n",
      "EPOCH: 376 TrainLoss: 0.036956826096346033 Accuracy: 0.7854285714285715\n",
      "EPOCH: 377 TrainLoss: 0.03687728453952945 Accuracy: 0.7916428571428571\n",
      "EPOCH: 378 TrainLoss: 0.03683774078413542 Accuracy: 0.7865\n",
      "EPOCH: 379 TrainLoss: 0.036760267799874916 Accuracy: 0.7927857142857143\n",
      "EPOCH: 380 TrainLoss: 0.03672026004221938 Accuracy: 0.7877142857142857\n",
      "EPOCH: 381 TrainLoss: 0.03664388909749058 Accuracy: 0.7939285714285714\n",
      "EPOCH: 382 TrainLoss: 0.03660331264509162 Accuracy: 0.7881428571428571\n",
      "EPOCH: 383 TrainLoss: 0.03652741637815923 Accuracy: 0.7957857142857143\n",
      "EPOCH: 384 TrainLoss: 0.03648562703230586 Accuracy: 0.7899285714285714\n",
      "EPOCH: 385 TrainLoss: 0.03641009602163387 Accuracy: 0.7974285714285714\n",
      "EPOCH: 386 TrainLoss: 0.03636725210229464 Accuracy: 0.7903571428571429\n",
      "EPOCH: 387 TrainLoss: 0.036293243052643795 Accuracy: 0.799\n",
      "EPOCH: 388 TrainLoss: 0.03625019682041887 Accuracy: 0.7916428571428571\n",
      "EPOCH: 389 TrainLoss: 0.03617842338543935 Accuracy: 0.7999285714285714\n",
      "EPOCH: 390 TrainLoss: 0.03613632270173675 Accuracy: 0.7927142857142857\n",
      "EPOCH: 391 TrainLoss: 0.03606552516957416 Accuracy: 0.8005714285714286\n",
      "EPOCH: 392 TrainLoss: 0.03602295141155688 Accuracy: 0.7942142857142858\n",
      "EPOCH: 393 TrainLoss: 0.03595245625504102 Accuracy: 0.8012142857142858\n",
      "EPOCH: 394 TrainLoss: 0.03590942617477564 Accuracy: 0.7955\n",
      "EPOCH: 395 TrainLoss: 0.035840994673868476 Accuracy: 0.8023571428571429\n",
      "EPOCH: 396 TrainLoss: 0.03579688293302919 Accuracy: 0.7963571428571429\n",
      "EPOCH: 397 TrainLoss: 0.035728621625716606 Accuracy: 0.8032142857142858\n",
      "EPOCH: 398 TrainLoss: 0.035684173689778664 Accuracy: 0.7976428571428571\n",
      "EPOCH: 399 TrainLoss: 0.03561672079565086 Accuracy: 0.8041428571428572\n",
      "EPOCH: 400 TrainLoss: 0.035572096172610285 Accuracy: 0.7987857142857143\n",
      "EPOCH: 401 TrainLoss: 0.03550553100561094 Accuracy: 0.8052857142857143\n",
      "EPOCH: 402 TrainLoss: 0.03546084539757411 Accuracy: 0.7995714285714286\n",
      "EPOCH: 403 TrainLoss: 0.03539557210539395 Accuracy: 0.806\n",
      "EPOCH: 404 TrainLoss: 0.03535056564019402 Accuracy: 0.8008571428571428\n",
      "EPOCH: 405 TrainLoss: 0.03528626034712129 Accuracy: 0.807\n",
      "EPOCH: 406 TrainLoss: 0.03524156451745916 Accuracy: 0.8015714285714286\n",
      "EPOCH: 407 TrainLoss: 0.03517790692130213 Accuracy: 0.808\n",
      "EPOCH: 408 TrainLoss: 0.0351326139328204 Accuracy: 0.8022142857142858\n",
      "EPOCH: 409 TrainLoss: 0.035070064937630104 Accuracy: 0.8088571428571428\n",
      "EPOCH: 410 TrainLoss: 0.03502439308753452 Accuracy: 0.8027142857142857\n",
      "EPOCH: 411 TrainLoss: 0.034961782200118995 Accuracy: 0.8094285714285714\n",
      "EPOCH: 412 TrainLoss: 0.03491479218770374 Accuracy: 0.8036428571428571\n",
      "EPOCH: 413 TrainLoss: 0.03485220263553086 Accuracy: 0.8101428571428572\n",
      "EPOCH: 414 TrainLoss: 0.0348047098976419 Accuracy: 0.8048571428571428\n",
      "EPOCH: 415 TrainLoss: 0.03474203398408848 Accuracy: 0.8108571428571428\n",
      "EPOCH: 416 TrainLoss: 0.03469387541990473 Accuracy: 0.8055714285714286\n",
      "EPOCH: 417 TrainLoss: 0.034632258498202825 Accuracy: 0.8117142857142857\n",
      "EPOCH: 418 TrainLoss: 0.03458406904029475 Accuracy: 0.8067857142857143\n",
      "EPOCH: 419 TrainLoss: 0.03452310091721701 Accuracy: 0.8127142857142857\n",
      "EPOCH: 420 TrainLoss: 0.0344740459364882 Accuracy: 0.8075714285714286\n",
      "EPOCH: 421 TrainLoss: 0.034413495490099213 Accuracy: 0.8133571428571429\n",
      "EPOCH: 422 TrainLoss: 0.03436413964403366 Accuracy: 0.809\n",
      "EPOCH: 423 TrainLoss: 0.03430507311474832 Accuracy: 0.8144285714285714\n",
      "EPOCH: 424 TrainLoss: 0.034256068146927124 Accuracy: 0.8101428571428572\n",
      "EPOCH: 425 TrainLoss: 0.034197641147095136 Accuracy: 0.8152857142857143\n",
      "EPOCH: 426 TrainLoss: 0.034148365447572195 Accuracy: 0.8111428571428572\n",
      "EPOCH: 427 TrainLoss: 0.03409051873558464 Accuracy: 0.8162857142857143\n",
      "EPOCH: 428 TrainLoss: 0.03404245056107657 Accuracy: 0.8123571428571429\n",
      "EPOCH: 429 TrainLoss: 0.033985395076218755 Accuracy: 0.8175\n",
      "EPOCH: 430 TrainLoss: 0.03393694113894426 Accuracy: 0.8135\n",
      "EPOCH: 431 TrainLoss: 0.033881064355283555 Accuracy: 0.8189285714285715\n",
      "EPOCH: 432 TrainLoss: 0.03383308335264458 Accuracy: 0.8147142857142857\n",
      "EPOCH: 433 TrainLoss: 0.03377814018955454 Accuracy: 0.8199285714285715\n",
      "EPOCH: 434 TrainLoss: 0.03373073942522342 Accuracy: 0.8154285714285714\n",
      "EPOCH: 435 TrainLoss: 0.033676500076927254 Accuracy: 0.8202857142857143\n",
      "EPOCH: 436 TrainLoss: 0.033629248125866686 Accuracy: 0.8160714285714286\n",
      "EPOCH: 437 TrainLoss: 0.03357563949051078 Accuracy: 0.8212142857142857\n",
      "EPOCH: 438 TrainLoss: 0.033529409957710164 Accuracy: 0.8172142857142857\n",
      "EPOCH: 439 TrainLoss: 0.03347677338619505 Accuracy: 0.8224285714285714\n",
      "EPOCH: 440 TrainLoss: 0.03343127182072641 Accuracy: 0.8182857142857143\n",
      "EPOCH: 441 TrainLoss: 0.03337847434555882 Accuracy: 0.8225714285714286\n",
      "EPOCH: 442 TrainLoss: 0.033332639549553074 Accuracy: 0.8187142857142857\n",
      "EPOCH: 443 TrainLoss: 0.03328045500656478 Accuracy: 0.8231428571428572\n",
      "EPOCH: 444 TrainLoss: 0.033235255805073624 Accuracy: 0.8197142857142857\n",
      "EPOCH: 445 TrainLoss: 0.03318400510779584 Accuracy: 0.8235714285714286\n",
      "EPOCH: 446 TrainLoss: 0.033139485627525986 Accuracy: 0.8209285714285715\n",
      "EPOCH: 447 TrainLoss: 0.03308831836844296 Accuracy: 0.8238571428571428\n",
      "EPOCH: 448 TrainLoss: 0.033044678872400604 Accuracy: 0.8216428571428571\n",
      "EPOCH: 449 TrainLoss: 0.03299293886485747 Accuracy: 0.8243571428571429\n",
      "EPOCH: 450 TrainLoss: 0.03294968082832258 Accuracy: 0.8219285714285715\n",
      "EPOCH: 451 TrainLoss: 0.032898610604623436 Accuracy: 0.8248571428571428\n",
      "EPOCH: 452 TrainLoss: 0.03285627441697398 Accuracy: 0.8225714285714286\n",
      "EPOCH: 453 TrainLoss: 0.03280457272074503 Accuracy: 0.826\n",
      "EPOCH: 454 TrainLoss: 0.032762615078570076 Accuracy: 0.8237857142857142\n",
      "EPOCH: 455 TrainLoss: 0.03271170316651397 Accuracy: 0.8267857142857142\n",
      "EPOCH: 456 TrainLoss: 0.0326694454175656 Accuracy: 0.8245714285714286\n",
      "EPOCH: 457 TrainLoss: 0.03261945031629424 Accuracy: 0.8276428571428571\n",
      "EPOCH: 458 TrainLoss: 0.032577122059398035 Accuracy: 0.8252142857142857\n",
      "EPOCH: 459 TrainLoss: 0.03252747521581044 Accuracy: 0.8281428571428572\n",
      "EPOCH: 460 TrainLoss: 0.03248473390640843 Accuracy: 0.8260714285714286\n",
      "EPOCH: 461 TrainLoss: 0.03243643761481438 Accuracy: 0.8285\n",
      "EPOCH: 462 TrainLoss: 0.03239379494171725 Accuracy: 0.8267857142857142\n",
      "EPOCH: 463 TrainLoss: 0.03234550063107267 Accuracy: 0.8295\n",
      "EPOCH: 464 TrainLoss: 0.03230330927123842 Accuracy: 0.8275\n",
      "EPOCH: 465 TrainLoss: 0.03225493463912539 Accuracy: 0.8301428571428572\n",
      "EPOCH: 466 TrainLoss: 0.032212517528423384 Accuracy: 0.8283571428571429\n",
      "EPOCH: 467 TrainLoss: 0.03216398214397441 Accuracy: 0.831\n",
      "EPOCH: 468 TrainLoss: 0.03212101041063213 Accuracy: 0.8293571428571429\n",
      "EPOCH: 469 TrainLoss: 0.03207196833633508 Accuracy: 0.8313571428571429\n",
      "EPOCH: 470 TrainLoss: 0.0320293911468152 Accuracy: 0.8299285714285715\n",
      "EPOCH: 471 TrainLoss: 0.031980638512024206 Accuracy: 0.8317142857142857\n",
      "EPOCH: 472 TrainLoss: 0.03193799271327525 Accuracy: 0.8305714285714285\n",
      "EPOCH: 473 TrainLoss: 0.031889728096508516 Accuracy: 0.8325714285714285\n",
      "EPOCH: 474 TrainLoss: 0.031846944971680245 Accuracy: 0.8315714285714285\n",
      "EPOCH: 475 TrainLoss: 0.03179920838637572 Accuracy: 0.8331428571428572\n",
      "EPOCH: 476 TrainLoss: 0.03175794146154279 Accuracy: 0.8321428571428572\n",
      "EPOCH: 477 TrainLoss: 0.03171101721468884 Accuracy: 0.8339285714285715\n",
      "EPOCH: 478 TrainLoss: 0.03167047156561341 Accuracy: 0.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 479 TrainLoss: 0.031624929121096094 Accuracy: 0.8345\n",
      "EPOCH: 480 TrainLoss: 0.031584931431382204 Accuracy: 0.8335714285714285\n",
      "EPOCH: 481 TrainLoss: 0.03153902740252027 Accuracy: 0.8352142857142857\n",
      "EPOCH: 482 TrainLoss: 0.031499098395512085 Accuracy: 0.8346428571428571\n",
      "EPOCH: 483 TrainLoss: 0.031453275163487435 Accuracy: 0.8359285714285715\n",
      "EPOCH: 484 TrainLoss: 0.03141379497317383 Accuracy: 0.8356428571428571\n",
      "EPOCH: 485 TrainLoss: 0.031367955430115425 Accuracy: 0.8367142857142857\n",
      "EPOCH: 486 TrainLoss: 0.03132903064342932 Accuracy: 0.8360714285714286\n",
      "EPOCH: 487 TrainLoss: 0.03128348735270498 Accuracy: 0.8374285714285714\n",
      "EPOCH: 488 TrainLoss: 0.031245190305882704 Accuracy: 0.8363571428571429\n",
      "EPOCH: 489 TrainLoss: 0.03120022722218375 Accuracy: 0.8383571428571429\n",
      "EPOCH: 490 TrainLoss: 0.031161429708996075 Accuracy: 0.8369285714285715\n",
      "EPOCH: 491 TrainLoss: 0.031116563581912733 Accuracy: 0.8387857142857142\n",
      "EPOCH: 492 TrainLoss: 0.03107779263627944 Accuracy: 0.8378571428571429\n",
      "EPOCH: 493 TrainLoss: 0.03103348461211617 Accuracy: 0.8392857142857143\n",
      "EPOCH: 494 TrainLoss: 0.030995249866043916 Accuracy: 0.8385714285714285\n",
      "EPOCH: 495 TrainLoss: 0.030951386020687616 Accuracy: 0.8399285714285715\n",
      "EPOCH: 496 TrainLoss: 0.030913377109577044 Accuracy: 0.8393571428571428\n",
      "EPOCH: 497 TrainLoss: 0.03086962825377908 Accuracy: 0.8401428571428572\n",
      "EPOCH: 498 TrainLoss: 0.030832380808556853 Accuracy: 0.8397857142857142\n",
      "EPOCH: 499 TrainLoss: 0.030788685237882335 Accuracy: 0.8408571428571429\n",
      "EPOCH: 500 TrainLoss: 0.03075098228433153 Accuracy: 0.8405\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "model = MLP_regressor()\n",
    "# 学習率\n",
    "lr = 0.1\n",
    "# 学習エポック数\n",
    "n_epoch = 500\n",
    "# 正答率\n",
    "acc_list = []\n",
    "acc_list_two=[]\n",
    "# n_epoch繰り返す\n",
    "for n in range(n_epoch):\n",
    "    # 訓練\n",
    "    x=x_train/255\n",
    "    t=t_train\n",
    "    y=model.forward(x)\n",
    "    train_loss=MSE(t,y)\n",
    "    model.backward(t,y)\n",
    "    model.optimize_GradientDecent(lr)\n",
    "    \n",
    "    # テスト\n",
    "    y = model.forward(x_test/255)\n",
    "    acc_list.append((y.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    print(\"EPOCH:\",n+1,\"TrainLoss:\",train_loss,\"Accuracy:\",acc_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて重ね数字のmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 TrainLoss: 0.20105210367251297 Accuracy: 0.12321428571428572\n",
      "EPOCH: 2 TrainLoss: 0.19374528565874122 Accuracy: 0.12442857142857143\n",
      "EPOCH: 3 TrainLoss: 0.18812947593351648 Accuracy: 0.12857142857142856\n",
      "EPOCH: 4 TrainLoss: 0.18369091668369053 Accuracy: 0.13085714285714287\n",
      "EPOCH: 5 TrainLoss: 0.1798918051558275 Accuracy: 0.13035714285714287\n",
      "EPOCH: 6 TrainLoss: 0.17647066614876117 Accuracy: 0.13792857142857143\n",
      "EPOCH: 7 TrainLoss: 0.17362953265138004 Accuracy: 0.13660714285714284\n",
      "EPOCH: 8 TrainLoss: 0.1716718767776861 Accuracy: 0.1372142857142857\n",
      "EPOCH: 9 TrainLoss: 0.16920556796090375 Accuracy: 0.13625\n",
      "EPOCH: 10 TrainLoss: 0.16745461122878597 Accuracy: 0.14042857142857143\n",
      "EPOCH: 11 TrainLoss: 0.1660145016800861 Accuracy: 0.14085714285714285\n",
      "EPOCH: 12 TrainLoss: 0.16425690959517528 Accuracy: 0.1429642857142857\n",
      "EPOCH: 13 TrainLoss: 0.16325316646371446 Accuracy: 0.1422142857142857\n",
      "EPOCH: 14 TrainLoss: 0.1621538727284072 Accuracy: 0.1452857142857143\n",
      "EPOCH: 15 TrainLoss: 0.16075631977494717 Accuracy: 0.14407142857142857\n",
      "EPOCH: 16 TrainLoss: 0.16003021121559366 Accuracy: 0.1455\n",
      "EPOCH: 17 TrainLoss: 0.15914567877078453 Accuracy: 0.14582142857142857\n",
      "EPOCH: 18 TrainLoss: 0.15822399333427942 Accuracy: 0.15203571428571427\n",
      "EPOCH: 19 TrainLoss: 0.1580084912955522 Accuracy: 0.15196428571428572\n",
      "EPOCH: 20 TrainLoss: 0.15739018811003355 Accuracy: 0.15721428571428572\n",
      "EPOCH: 21 TrainLoss: 0.15672562907640128 Accuracy: 0.15817857142857142\n",
      "EPOCH: 22 TrainLoss: 0.1562661693623651 Accuracy: 0.15957142857142856\n",
      "EPOCH: 23 TrainLoss: 0.15582536546361445 Accuracy: 0.15982142857142856\n",
      "EPOCH: 24 TrainLoss: 0.1553950845891936 Accuracy: 0.16378571428571428\n",
      "EPOCH: 25 TrainLoss: 0.15526581923788677 Accuracy: 0.16217857142857142\n",
      "EPOCH: 26 TrainLoss: 0.15467539119822407 Accuracy: 0.16292857142857142\n",
      "EPOCH: 27 TrainLoss: 0.1543988186817497 Accuracy: 0.1665\n",
      "EPOCH: 28 TrainLoss: 0.15413519615863974 Accuracy: 0.17539285714285716\n",
      "EPOCH: 29 TrainLoss: 0.15382774289839932 Accuracy: 0.17532142857142857\n",
      "EPOCH: 30 TrainLoss: 0.15377801637025262 Accuracy: 0.17792857142857144\n",
      "EPOCH: 31 TrainLoss: 0.15355439097350662 Accuracy: 0.1800357142857143\n",
      "EPOCH: 32 TrainLoss: 0.1531056394916186 Accuracy: 0.18021428571428572\n",
      "EPOCH: 33 TrainLoss: 0.1530058660518136 Accuracy: 0.18585714285714286\n",
      "EPOCH: 34 TrainLoss: 0.15284198360489726 Accuracy: 0.18828571428571428\n",
      "EPOCH: 35 TrainLoss: 0.1525965726995481 Accuracy: 0.18935714285714286\n",
      "EPOCH: 36 TrainLoss: 0.15247603192472411 Accuracy: 0.18964285714285714\n",
      "EPOCH: 37 TrainLoss: 0.15226492456785018 Accuracy: 0.19482142857142856\n",
      "EPOCH: 38 TrainLoss: 0.15223422621353086 Accuracy: 0.19989285714285715\n",
      "EPOCH: 39 TrainLoss: 0.15191746128570924 Accuracy: 0.19457142857142856\n",
      "EPOCH: 40 TrainLoss: 0.1517647827890606 Accuracy: 0.2017857142857143\n",
      "EPOCH: 41 TrainLoss: 0.15144474685124198 Accuracy: 0.2012142857142857\n",
      "EPOCH: 42 TrainLoss: 0.15148670398533595 Accuracy: 0.2055\n",
      "EPOCH: 43 TrainLoss: 0.15159794398551288 Accuracy: 0.20889285714285713\n",
      "EPOCH: 44 TrainLoss: 0.1512912528173458 Accuracy: 0.20975\n",
      "EPOCH: 45 TrainLoss: 0.15116064183148037 Accuracy: 0.20857142857142857\n",
      "EPOCH: 46 TrainLoss: 0.15085994959568008 Accuracy: 0.21692857142857142\n",
      "EPOCH: 47 TrainLoss: 0.15085293977316747 Accuracy: 0.21817857142857142\n",
      "EPOCH: 48 TrainLoss: 0.1507187626418405 Accuracy: 0.21510714285714286\n",
      "EPOCH: 49 TrainLoss: 0.1507470014222209 Accuracy: 0.21967857142857142\n",
      "EPOCH: 50 TrainLoss: 0.15051342833129527 Accuracy: 0.22307142857142856\n",
      "EPOCH: 51 TrainLoss: 0.15043948055599093 Accuracy: 0.22185714285714286\n",
      "EPOCH: 52 TrainLoss: 0.15025838675945283 Accuracy: 0.22114285714285714\n",
      "EPOCH: 53 TrainLoss: 0.15012204805426088 Accuracy: 0.22603571428571428\n",
      "EPOCH: 54 TrainLoss: 0.15012754772695946 Accuracy: 0.2245\n",
      "EPOCH: 55 TrainLoss: 0.14998120114382418 Accuracy: 0.22507142857142856\n",
      "EPOCH: 56 TrainLoss: 0.14999623965153966 Accuracy: 0.2235\n",
      "EPOCH: 57 TrainLoss: 0.14980418420942598 Accuracy: 0.22914285714285715\n",
      "EPOCH: 58 TrainLoss: 0.14962702585829502 Accuracy: 0.22478571428571428\n",
      "EPOCH: 59 TrainLoss: 0.14946365336746834 Accuracy: 0.2322857142857143\n",
      "EPOCH: 60 TrainLoss: 0.14944181572462498 Accuracy: 0.23175\n",
      "EPOCH: 61 TrainLoss: 0.14926493597850626 Accuracy: 0.23192857142857143\n",
      "EPOCH: 62 TrainLoss: 0.1493011633740007 Accuracy: 0.23417857142857143\n",
      "EPOCH: 63 TrainLoss: 0.14920570437719965 Accuracy: 0.2362857142857143\n",
      "EPOCH: 64 TrainLoss: 0.14902774768403665 Accuracy: 0.23335714285714285\n",
      "EPOCH: 65 TrainLoss: 0.14886597701732457 Accuracy: 0.2355357142857143\n",
      "EPOCH: 66 TrainLoss: 0.14895433283136209 Accuracy: 0.23635714285714285\n",
      "EPOCH: 67 TrainLoss: 0.14864440888666353 Accuracy: 0.2390357142857143\n",
      "EPOCH: 68 TrainLoss: 0.14874341488412124 Accuracy: 0.23925\n",
      "EPOCH: 69 TrainLoss: 0.14852303769184796 Accuracy: 0.24160714285714285\n",
      "EPOCH: 70 TrainLoss: 0.14850418629654746 Accuracy: 0.2397857142857143\n",
      "EPOCH: 71 TrainLoss: 0.14851390714741988 Accuracy: 0.24314285714285713\n",
      "EPOCH: 72 TrainLoss: 0.1482312660514862 Accuracy: 0.23985714285714285\n",
      "EPOCH: 73 TrainLoss: 0.14816585265881352 Accuracy: 0.24535714285714286\n",
      "EPOCH: 74 TrainLoss: 0.14815974539318652 Accuracy: 0.24039285714285713\n",
      "EPOCH: 75 TrainLoss: 0.1479904897148544 Accuracy: 0.24957142857142858\n",
      "EPOCH: 76 TrainLoss: 0.14792638692490936 Accuracy: 0.24667857142857144\n",
      "EPOCH: 77 TrainLoss: 0.14784591493787055 Accuracy: 0.24742857142857144\n",
      "EPOCH: 78 TrainLoss: 0.1477976545491907 Accuracy: 0.24839285714285714\n",
      "EPOCH: 79 TrainLoss: 0.14773643076447995 Accuracy: 0.24885714285714286\n",
      "EPOCH: 80 TrainLoss: 0.1475569769426446 Accuracy: 0.24864285714285714\n",
      "EPOCH: 81 TrainLoss: 0.14758899860662275 Accuracy: 0.2512142857142857\n",
      "EPOCH: 82 TrainLoss: 0.1474816829898138 Accuracy: 0.25314285714285717\n",
      "EPOCH: 83 TrainLoss: 0.14734616568777797 Accuracy: 0.2545357142857143\n",
      "EPOCH: 84 TrainLoss: 0.1471683956752137 Accuracy: 0.2562142857142857\n",
      "EPOCH: 85 TrainLoss: 0.14716796402171048 Accuracy: 0.2572857142857143\n",
      "EPOCH: 86 TrainLoss: 0.14712887804929 Accuracy: 0.25310714285714286\n",
      "EPOCH: 87 TrainLoss: 0.1468379928312286 Accuracy: 0.2524642857142857\n",
      "EPOCH: 88 TrainLoss: 0.14690219783902647 Accuracy: 0.25442857142857145\n",
      "EPOCH: 89 TrainLoss: 0.1467781779438304 Accuracy: 0.2566428571428571\n",
      "EPOCH: 90 TrainLoss: 0.14657349819806495 Accuracy: 0.2589642857142857\n",
      "EPOCH: 91 TrainLoss: 0.14664961334903304 Accuracy: 0.2581785714285714\n",
      "EPOCH: 92 TrainLoss: 0.14641263538875823 Accuracy: 0.26060714285714287\n",
      "EPOCH: 93 TrainLoss: 0.14650266323158295 Accuracy: 0.26185714285714284\n",
      "EPOCH: 94 TrainLoss: 0.1463203545288182 Accuracy: 0.263\n",
      "EPOCH: 95 TrainLoss: 0.14635963303259414 Accuracy: 0.25742857142857145\n",
      "EPOCH: 96 TrainLoss: 0.1461863616618647 Accuracy: 0.2646428571428571\n",
      "EPOCH: 97 TrainLoss: 0.1458742790242791 Accuracy: 0.26457142857142857\n",
      "EPOCH: 98 TrainLoss: 0.14595936615217897 Accuracy: 0.2643214285714286\n",
      "EPOCH: 99 TrainLoss: 0.14588748515625333 Accuracy: 0.2641428571428571\n",
      "EPOCH: 100 TrainLoss: 0.14591933643764982 Accuracy: 0.2681071428571429\n",
      "EPOCH: 101 TrainLoss: 0.14560446576701927 Accuracy: 0.26882142857142854\n",
      "EPOCH: 102 TrainLoss: 0.14560806165339404 Accuracy: 0.2636071428571429\n",
      "EPOCH: 103 TrainLoss: 0.14558299790746587 Accuracy: 0.26635714285714285\n",
      "EPOCH: 104 TrainLoss: 0.1455040782931015 Accuracy: 0.27103571428571427\n",
      "EPOCH: 105 TrainLoss: 0.1454041153407938 Accuracy: 0.2661071428571429\n",
      "EPOCH: 106 TrainLoss: 0.14518051908491955 Accuracy: 0.27139285714285716\n",
      "EPOCH: 107 TrainLoss: 0.14507148174298623 Accuracy: 0.26807142857142857\n",
      "EPOCH: 108 TrainLoss: 0.14516878008094058 Accuracy: 0.2705\n",
      "EPOCH: 109 TrainLoss: 0.1448920976943269 Accuracy: 0.27382142857142855\n",
      "EPOCH: 110 TrainLoss: 0.14510567774197516 Accuracy: 0.2735714285714286\n",
      "EPOCH: 111 TrainLoss: 0.14490933506105544 Accuracy: 0.2707857142857143\n",
      "EPOCH: 112 TrainLoss: 0.14491202084934868 Accuracy: 0.27017857142857143\n",
      "EPOCH: 113 TrainLoss: 0.14465090680099463 Accuracy: 0.27225\n",
      "EPOCH: 114 TrainLoss: 0.1446858201822996 Accuracy: 0.27564285714285713\n",
      "EPOCH: 115 TrainLoss: 0.14442814623713687 Accuracy: 0.27175\n",
      "EPOCH: 116 TrainLoss: 0.14446015845321303 Accuracy: 0.2750714285714286\n",
      "EPOCH: 117 TrainLoss: 0.14457906571269477 Accuracy: 0.2751071428571429\n",
      "EPOCH: 118 TrainLoss: 0.1442214303810973 Accuracy: 0.2770714285714286\n",
      "EPOCH: 119 TrainLoss: 0.14408407253310576 Accuracy: 0.27775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 120 TrainLoss: 0.14422054592235964 Accuracy: 0.28017857142857144\n",
      "EPOCH: 121 TrainLoss: 0.1440679910171772 Accuracy: 0.2797857142857143\n",
      "EPOCH: 122 TrainLoss: 0.14393118152846324 Accuracy: 0.2802857142857143\n",
      "EPOCH: 123 TrainLoss: 0.14392637667873032 Accuracy: 0.2785\n",
      "EPOCH: 124 TrainLoss: 0.1438295603866927 Accuracy: 0.27932142857142855\n",
      "EPOCH: 125 TrainLoss: 0.1437422840766338 Accuracy: 0.2832142857142857\n",
      "EPOCH: 126 TrainLoss: 0.14384042003708244 Accuracy: 0.283\n",
      "EPOCH: 127 TrainLoss: 0.1435281248143003 Accuracy: 0.2857857142857143\n",
      "EPOCH: 128 TrainLoss: 0.14354528317868434 Accuracy: 0.28075\n",
      "EPOCH: 129 TrainLoss: 0.14360103616073372 Accuracy: 0.2859285714285714\n",
      "EPOCH: 130 TrainLoss: 0.1434516209148388 Accuracy: 0.2908928571428571\n",
      "EPOCH: 131 TrainLoss: 0.14321564525086963 Accuracy: 0.28564285714285714\n",
      "EPOCH: 132 TrainLoss: 0.14312274806074005 Accuracy: 0.2859285714285714\n",
      "EPOCH: 133 TrainLoss: 0.14305342996081707 Accuracy: 0.2869285714285714\n",
      "EPOCH: 134 TrainLoss: 0.14306769736882818 Accuracy: 0.2835714285714286\n",
      "EPOCH: 135 TrainLoss: 0.14297710262969102 Accuracy: 0.2809642857142857\n",
      "EPOCH: 136 TrainLoss: 0.14280864470921414 Accuracy: 0.28775\n",
      "EPOCH: 137 TrainLoss: 0.1427359710307355 Accuracy: 0.2875714285714286\n",
      "EPOCH: 138 TrainLoss: 0.14266094432678925 Accuracy: 0.2887142857142857\n",
      "EPOCH: 139 TrainLoss: 0.1425895760176105 Accuracy: 0.28510714285714284\n",
      "EPOCH: 140 TrainLoss: 0.14245268928268717 Accuracy: 0.29182142857142856\n",
      "EPOCH: 141 TrainLoss: 0.1424210432000869 Accuracy: 0.2960714285714286\n",
      "EPOCH: 142 TrainLoss: 0.1424044275528826 Accuracy: 0.29132142857142856\n",
      "EPOCH: 143 TrainLoss: 0.1421402401872237 Accuracy: 0.2917142857142857\n",
      "EPOCH: 144 TrainLoss: 0.14233397413155705 Accuracy: 0.29282142857142857\n",
      "EPOCH: 145 TrainLoss: 0.1420593226703439 Accuracy: 0.29478571428571426\n",
      "EPOCH: 146 TrainLoss: 0.14219210638405544 Accuracy: 0.2938928571428571\n",
      "EPOCH: 147 TrainLoss: 0.1420629459859735 Accuracy: 0.2977142857142857\n",
      "EPOCH: 148 TrainLoss: 0.14177385165668824 Accuracy: 0.29542857142857143\n",
      "EPOCH: 149 TrainLoss: 0.14171428502140032 Accuracy: 0.29596428571428574\n",
      "EPOCH: 150 TrainLoss: 0.1418056751229413 Accuracy: 0.29828571428571427\n",
      "EPOCH: 151 TrainLoss: 0.14162722168146516 Accuracy: 0.29978571428571427\n",
      "EPOCH: 152 TrainLoss: 0.1413933423566635 Accuracy: 0.29846428571428574\n",
      "EPOCH: 153 TrainLoss: 0.14150027654269776 Accuracy: 0.2950357142857143\n",
      "EPOCH: 154 TrainLoss: 0.14147818922158456 Accuracy: 0.29332142857142857\n",
      "EPOCH: 155 TrainLoss: 0.14130594635672974 Accuracy: 0.29714285714285715\n",
      "EPOCH: 156 TrainLoss: 0.14132248346213305 Accuracy: 0.30096428571428574\n",
      "EPOCH: 157 TrainLoss: 0.14093109615812535 Accuracy: 0.2983928571428571\n",
      "EPOCH: 158 TrainLoss: 0.14112637622658017 Accuracy: 0.3\n",
      "EPOCH: 159 TrainLoss: 0.14099372150983125 Accuracy: 0.2991785714285714\n",
      "EPOCH: 160 TrainLoss: 0.14097069291621903 Accuracy: 0.30089285714285713\n",
      "EPOCH: 161 TrainLoss: 0.14093197303464444 Accuracy: 0.3006785714285714\n",
      "EPOCH: 162 TrainLoss: 0.14079506432527147 Accuracy: 0.3008571428571429\n",
      "EPOCH: 163 TrainLoss: 0.14067575502312005 Accuracy: 0.30325\n",
      "EPOCH: 164 TrainLoss: 0.14058371261913977 Accuracy: 0.3032142857142857\n",
      "EPOCH: 165 TrainLoss: 0.14054960984282802 Accuracy: 0.3028571428571429\n",
      "EPOCH: 166 TrainLoss: 0.1404943243422601 Accuracy: 0.30225\n",
      "EPOCH: 167 TrainLoss: 0.1402760438858808 Accuracy: 0.30392857142857144\n",
      "EPOCH: 168 TrainLoss: 0.1404633917074762 Accuracy: 0.30528571428571427\n",
      "EPOCH: 169 TrainLoss: 0.1402810327326241 Accuracy: 0.3056785714285714\n",
      "EPOCH: 170 TrainLoss: 0.14007143079212883 Accuracy: 0.3092857142857143\n",
      "EPOCH: 171 TrainLoss: 0.14001185101866348 Accuracy: 0.3091785714285714\n",
      "EPOCH: 172 TrainLoss: 0.13993915708939658 Accuracy: 0.3067142857142857\n",
      "EPOCH: 173 TrainLoss: 0.1397385587175178 Accuracy: 0.30407142857142855\n",
      "EPOCH: 174 TrainLoss: 0.13987690482281634 Accuracy: 0.31342857142857145\n",
      "EPOCH: 175 TrainLoss: 0.13973838262926583 Accuracy: 0.30528571428571427\n",
      "EPOCH: 176 TrainLoss: 0.13948406853858908 Accuracy: 0.3115357142857143\n",
      "EPOCH: 177 TrainLoss: 0.1396130486237323 Accuracy: 0.30875\n",
      "EPOCH: 178 TrainLoss: 0.1393268925987315 Accuracy: 0.31\n",
      "EPOCH: 179 TrainLoss: 0.13934298314522342 Accuracy: 0.3135357142857143\n",
      "EPOCH: 180 TrainLoss: 0.1390762241698477 Accuracy: 0.3117857142857143\n",
      "EPOCH: 181 TrainLoss: 0.1393192280678477 Accuracy: 0.3157142857142857\n",
      "EPOCH: 182 TrainLoss: 0.1391283986304398 Accuracy: 0.3145\n",
      "EPOCH: 183 TrainLoss: 0.13905779762233067 Accuracy: 0.31557142857142856\n",
      "EPOCH: 184 TrainLoss: 0.1389201219397451 Accuracy: 0.31260714285714286\n",
      "EPOCH: 185 TrainLoss: 0.13874627376852558 Accuracy: 0.31185714285714283\n",
      "EPOCH: 186 TrainLoss: 0.13882386241081587 Accuracy: 0.31607142857142856\n",
      "EPOCH: 187 TrainLoss: 0.13876302915186114 Accuracy: 0.31692857142857145\n",
      "EPOCH: 188 TrainLoss: 0.13849212724001073 Accuracy: 0.31360714285714286\n",
      "EPOCH: 189 TrainLoss: 0.13858516946040375 Accuracy: 0.3139642857142857\n",
      "EPOCH: 190 TrainLoss: 0.13829723141996264 Accuracy: 0.31792857142857145\n",
      "EPOCH: 191 TrainLoss: 0.13843809820117836 Accuracy: 0.3131785714285714\n",
      "EPOCH: 192 TrainLoss: 0.13829814705406837 Accuracy: 0.31857142857142856\n",
      "EPOCH: 193 TrainLoss: 0.13821130778222107 Accuracy: 0.3228214285714286\n",
      "EPOCH: 194 TrainLoss: 0.13823359486470246 Accuracy: 0.3191428571428571\n",
      "EPOCH: 195 TrainLoss: 0.13809745691037423 Accuracy: 0.3190357142857143\n",
      "EPOCH: 196 TrainLoss: 0.13796466052067025 Accuracy: 0.3161785714285714\n",
      "EPOCH: 197 TrainLoss: 0.1379501156859931 Accuracy: 0.32321428571428573\n",
      "EPOCH: 198 TrainLoss: 0.13789615564586669 Accuracy: 0.3199642857142857\n",
      "EPOCH: 199 TrainLoss: 0.13796230550355418 Accuracy: 0.31889285714285714\n",
      "EPOCH: 200 TrainLoss: 0.13775651831040958 Accuracy: 0.3268214285714286\n",
      "EPOCH: 201 TrainLoss: 0.13770705245920623 Accuracy: 0.32207142857142856\n",
      "EPOCH: 202 TrainLoss: 0.1374811811873715 Accuracy: 0.32321428571428573\n",
      "EPOCH: 203 TrainLoss: 0.13734604755130697 Accuracy: 0.3249642857142857\n",
      "EPOCH: 204 TrainLoss: 0.13727834925471213 Accuracy: 0.32575\n",
      "EPOCH: 205 TrainLoss: 0.13725755965480874 Accuracy: 0.3222857142857143\n",
      "EPOCH: 206 TrainLoss: 0.1372638679761221 Accuracy: 0.3273214285714286\n",
      "EPOCH: 207 TrainLoss: 0.1372229755889469 Accuracy: 0.32435714285714284\n",
      "EPOCH: 208 TrainLoss: 0.1370841470615672 Accuracy: 0.3227857142857143\n",
      "EPOCH: 209 TrainLoss: 0.1368548500472854 Accuracy: 0.32285714285714284\n",
      "EPOCH: 210 TrainLoss: 0.136838146338012 Accuracy: 0.32689285714285715\n",
      "EPOCH: 211 TrainLoss: 0.13669250432786298 Accuracy: 0.32721428571428574\n",
      "EPOCH: 212 TrainLoss: 0.13693118658801534 Accuracy: 0.32857142857142857\n",
      "EPOCH: 213 TrainLoss: 0.13672303360254412 Accuracy: 0.32835714285714285\n",
      "EPOCH: 214 TrainLoss: 0.13658942060316384 Accuracy: 0.3291071428571429\n",
      "EPOCH: 215 TrainLoss: 0.13656053029546955 Accuracy: 0.32525\n",
      "EPOCH: 216 TrainLoss: 0.13654630663542316 Accuracy: 0.3281428571428571\n",
      "EPOCH: 217 TrainLoss: 0.13646861602309687 Accuracy: 0.33453571428571427\n",
      "EPOCH: 218 TrainLoss: 0.13624373163780026 Accuracy: 0.3315\n",
      "EPOCH: 219 TrainLoss: 0.1360912149727853 Accuracy: 0.32889285714285715\n",
      "EPOCH: 220 TrainLoss: 0.1360382793602601 Accuracy: 0.33739285714285716\n",
      "EPOCH: 221 TrainLoss: 0.1359680586660839 Accuracy: 0.33482142857142855\n",
      "EPOCH: 222 TrainLoss: 0.13589676772324813 Accuracy: 0.32871428571428574\n",
      "EPOCH: 223 TrainLoss: 0.13592142811392868 Accuracy: 0.33225\n",
      "EPOCH: 224 TrainLoss: 0.13588625683578456 Accuracy: 0.3369642857142857\n",
      "EPOCH: 225 TrainLoss: 0.13565515956571791 Accuracy: 0.33514285714285713\n",
      "EPOCH: 226 TrainLoss: 0.1355033684347397 Accuracy: 0.33221428571428574\n",
      "EPOCH: 227 TrainLoss: 0.13564514007075695 Accuracy: 0.33832142857142855\n",
      "EPOCH: 228 TrainLoss: 0.1356561393094565 Accuracy: 0.33414285714285713\n",
      "EPOCH: 229 TrainLoss: 0.13518482286180136 Accuracy: 0.3376071428571429\n",
      "EPOCH: 230 TrainLoss: 0.13535291639688551 Accuracy: 0.34032142857142855\n",
      "EPOCH: 231 TrainLoss: 0.135128831806194 Accuracy: 0.33453571428571427\n",
      "EPOCH: 232 TrainLoss: 0.1350736210162466 Accuracy: 0.3400357142857143\n",
      "EPOCH: 233 TrainLoss: 0.13508985370947263 Accuracy: 0.34217857142857144\n",
      "EPOCH: 234 TrainLoss: 0.13478813752188543 Accuracy: 0.3375357142857143\n",
      "EPOCH: 235 TrainLoss: 0.13499019106840127 Accuracy: 0.34264285714285714\n",
      "EPOCH: 236 TrainLoss: 0.13466735182049314 Accuracy: 0.34075\n",
      "EPOCH: 237 TrainLoss: 0.13465272591720895 Accuracy: 0.3425357142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 238 TrainLoss: 0.13461229791378207 Accuracy: 0.3400714285714286\n",
      "EPOCH: 239 TrainLoss: 0.134600406378319 Accuracy: 0.3415\n",
      "EPOCH: 240 TrainLoss: 0.13453584091110726 Accuracy: 0.3435357142857143\n",
      "EPOCH: 241 TrainLoss: 0.13438020383561092 Accuracy: 0.3391071428571429\n",
      "EPOCH: 242 TrainLoss: 0.1343791771439057 Accuracy: 0.3409285714285714\n",
      "EPOCH: 243 TrainLoss: 0.1343289411321169 Accuracy: 0.3424642857142857\n",
      "EPOCH: 244 TrainLoss: 0.13401969718835632 Accuracy: 0.3434285714285714\n",
      "EPOCH: 245 TrainLoss: 0.13379439594207412 Accuracy: 0.3432857142857143\n",
      "EPOCH: 246 TrainLoss: 0.13377791856349766 Accuracy: 0.34214285714285714\n",
      "EPOCH: 247 TrainLoss: 0.13390125345666307 Accuracy: 0.34935714285714287\n",
      "EPOCH: 248 TrainLoss: 0.13402805739021167 Accuracy: 0.3479285714285714\n",
      "EPOCH: 249 TrainLoss: 0.13362800605144487 Accuracy: 0.34610714285714284\n",
      "EPOCH: 250 TrainLoss: 0.13364248824183922 Accuracy: 0.3424285714285714\n",
      "EPOCH: 251 TrainLoss: 0.13334795438639022 Accuracy: 0.34367857142857144\n",
      "EPOCH: 252 TrainLoss: 0.13357410705003936 Accuracy: 0.3467857142857143\n",
      "EPOCH: 253 TrainLoss: 0.13320584271983088 Accuracy: 0.35064285714285715\n",
      "EPOCH: 254 TrainLoss: 0.13336656978080208 Accuracy: 0.34639285714285717\n",
      "EPOCH: 255 TrainLoss: 0.13325820887913356 Accuracy: 0.3454642857142857\n",
      "EPOCH: 256 TrainLoss: 0.133118546781982 Accuracy: 0.3464642857142857\n",
      "EPOCH: 257 TrainLoss: 0.13312405517841178 Accuracy: 0.3449642857142857\n",
      "EPOCH: 258 TrainLoss: 0.13309233998583828 Accuracy: 0.35082142857142856\n",
      "EPOCH: 259 TrainLoss: 0.13267782440810516 Accuracy: 0.35114285714285715\n",
      "EPOCH: 260 TrainLoss: 0.1328258303398778 Accuracy: 0.34717857142857145\n",
      "EPOCH: 261 TrainLoss: 0.1328128046852247 Accuracy: 0.35435714285714287\n",
      "EPOCH: 262 TrainLoss: 0.1325328977889691 Accuracy: 0.3495\n",
      "EPOCH: 263 TrainLoss: 0.13253065731917335 Accuracy: 0.34682142857142856\n",
      "EPOCH: 264 TrainLoss: 0.13268302713621655 Accuracy: 0.35325\n",
      "EPOCH: 265 TrainLoss: 0.13264942756616746 Accuracy: 0.35496428571428573\n",
      "EPOCH: 266 TrainLoss: 0.1324424914039224 Accuracy: 0.34975\n",
      "EPOCH: 267 TrainLoss: 0.13237539528589928 Accuracy: 0.35882142857142857\n",
      "EPOCH: 268 TrainLoss: 0.13220756717542007 Accuracy: 0.35260714285714284\n",
      "EPOCH: 269 TrainLoss: 0.1320003225897985 Accuracy: 0.3577142857142857\n",
      "EPOCH: 270 TrainLoss: 0.13210702562544935 Accuracy: 0.3517142857142857\n",
      "EPOCH: 271 TrainLoss: 0.13184658110894854 Accuracy: 0.35246428571428573\n",
      "EPOCH: 272 TrainLoss: 0.13202271388328649 Accuracy: 0.3520357142857143\n",
      "EPOCH: 273 TrainLoss: 0.13175604998403803 Accuracy: 0.35560714285714284\n",
      "EPOCH: 274 TrainLoss: 0.1317828348902531 Accuracy: 0.35942857142857143\n",
      "EPOCH: 275 TrainLoss: 0.1317254587352433 Accuracy: 0.358\n",
      "EPOCH: 276 TrainLoss: 0.13146795439129938 Accuracy: 0.3559285714285714\n",
      "EPOCH: 277 TrainLoss: 0.13155416233800116 Accuracy: 0.3520714285714286\n",
      "EPOCH: 278 TrainLoss: 0.1310627989719035 Accuracy: 0.36232142857142857\n",
      "EPOCH: 279 TrainLoss: 0.13117016114018373 Accuracy: 0.3605\n",
      "EPOCH: 280 TrainLoss: 0.1313291018431945 Accuracy: 0.35775\n",
      "EPOCH: 281 TrainLoss: 0.13113686057557356 Accuracy: 0.35832142857142857\n",
      "EPOCH: 282 TrainLoss: 0.13119522271751086 Accuracy: 0.3573928571428571\n",
      "EPOCH: 283 TrainLoss: 0.1311589476807699 Accuracy: 0.3578571428571429\n",
      "EPOCH: 284 TrainLoss: 0.13074636083508392 Accuracy: 0.3615357142857143\n",
      "EPOCH: 285 TrainLoss: 0.13092849301402174 Accuracy: 0.36175\n",
      "EPOCH: 286 TrainLoss: 0.13073068712028835 Accuracy: 0.36278571428571427\n",
      "EPOCH: 287 TrainLoss: 0.13055724456554982 Accuracy: 0.36439285714285713\n",
      "EPOCH: 288 TrainLoss: 0.13044252307721094 Accuracy: 0.36539285714285713\n",
      "EPOCH: 289 TrainLoss: 0.13064928095007786 Accuracy: 0.35846428571428574\n",
      "EPOCH: 290 TrainLoss: 0.13056694073186176 Accuracy: 0.36196428571428574\n",
      "EPOCH: 291 TrainLoss: 0.13038472593977773 Accuracy: 0.36278571428571427\n",
      "EPOCH: 292 TrainLoss: 0.13021582279139313 Accuracy: 0.36428571428571427\n",
      "EPOCH: 293 TrainLoss: 0.1302918774276334 Accuracy: 0.36378571428571427\n",
      "EPOCH: 294 TrainLoss: 0.13037314000112057 Accuracy: 0.36478571428571427\n",
      "EPOCH: 295 TrainLoss: 0.1298129845684788 Accuracy: 0.36742857142857144\n",
      "EPOCH: 296 TrainLoss: 0.12991877188878362 Accuracy: 0.36364285714285716\n",
      "EPOCH: 297 TrainLoss: 0.12993155043368665 Accuracy: 0.3665\n",
      "EPOCH: 298 TrainLoss: 0.12980286548662434 Accuracy: 0.3682142857142857\n",
      "EPOCH: 299 TrainLoss: 0.1298002425311971 Accuracy: 0.3668214285714286\n",
      "EPOCH: 300 TrainLoss: 0.1296956117669925 Accuracy: 0.37089285714285714\n",
      "EPOCH: 301 TrainLoss: 0.1294073467395408 Accuracy: 0.36625\n",
      "EPOCH: 302 TrainLoss: 0.12939694566745358 Accuracy: 0.36564285714285716\n",
      "EPOCH: 303 TrainLoss: 0.1292987833790558 Accuracy: 0.3661785714285714\n",
      "EPOCH: 304 TrainLoss: 0.12931156302374403 Accuracy: 0.3628571428571429\n",
      "EPOCH: 305 TrainLoss: 0.12921107279843347 Accuracy: 0.3697857142857143\n",
      "EPOCH: 306 TrainLoss: 0.12922079660807617 Accuracy: 0.36557142857142855\n",
      "EPOCH: 307 TrainLoss: 0.1291277920441156 Accuracy: 0.3628571428571429\n",
      "EPOCH: 308 TrainLoss: 0.12906756520342585 Accuracy: 0.36725\n",
      "EPOCH: 309 TrainLoss: 0.12921549752964517 Accuracy: 0.3685\n",
      "EPOCH: 310 TrainLoss: 0.12884290129860623 Accuracy: 0.37085714285714283\n",
      "EPOCH: 311 TrainLoss: 0.1287777147196179 Accuracy: 0.37285714285714283\n",
      "EPOCH: 312 TrainLoss: 0.12861167847217803 Accuracy: 0.37142857142857144\n",
      "EPOCH: 313 TrainLoss: 0.1287754529360209 Accuracy: 0.374\n",
      "EPOCH: 314 TrainLoss: 0.12859518150760696 Accuracy: 0.37207142857142855\n",
      "EPOCH: 315 TrainLoss: 0.12832866180361674 Accuracy: 0.3683571428571429\n",
      "EPOCH: 316 TrainLoss: 0.12856723148224067 Accuracy: 0.37110714285714286\n",
      "EPOCH: 317 TrainLoss: 0.12845825640173816 Accuracy: 0.3697142857142857\n",
      "EPOCH: 318 TrainLoss: 0.12829955344637142 Accuracy: 0.37457142857142856\n",
      "EPOCH: 319 TrainLoss: 0.1282777200792387 Accuracy: 0.37539285714285714\n",
      "EPOCH: 320 TrainLoss: 0.1281645044444787 Accuracy: 0.37814285714285717\n",
      "EPOCH: 321 TrainLoss: 0.12792387029445684 Accuracy: 0.37360714285714286\n",
      "EPOCH: 322 TrainLoss: 0.12808156628157194 Accuracy: 0.3729642857142857\n",
      "EPOCH: 323 TrainLoss: 0.12784018858358595 Accuracy: 0.37064285714285716\n",
      "EPOCH: 324 TrainLoss: 0.12781724182272408 Accuracy: 0.3755\n",
      "EPOCH: 325 TrainLoss: 0.12779025506935932 Accuracy: 0.3742142857142857\n",
      "EPOCH: 326 TrainLoss: 0.12771620360290656 Accuracy: 0.3786785714285714\n",
      "EPOCH: 327 TrainLoss: 0.1276804408791554 Accuracy: 0.37335714285714283\n",
      "EPOCH: 328 TrainLoss: 0.127511578895199 Accuracy: 0.3765\n",
      "EPOCH: 329 TrainLoss: 0.12731777683711507 Accuracy: 0.37042857142857144\n",
      "EPOCH: 330 TrainLoss: 0.12741957425236905 Accuracy: 0.37975\n",
      "EPOCH: 331 TrainLoss: 0.12748294864190607 Accuracy: 0.37757142857142856\n",
      "EPOCH: 332 TrainLoss: 0.1272190316930243 Accuracy: 0.3785\n",
      "EPOCH: 333 TrainLoss: 0.12720448429746525 Accuracy: 0.37757142857142856\n",
      "EPOCH: 334 TrainLoss: 0.12703301871109793 Accuracy: 0.3765\n",
      "EPOCH: 335 TrainLoss: 0.1269910402281941 Accuracy: 0.37525\n",
      "EPOCH: 336 TrainLoss: 0.12673358316438488 Accuracy: 0.3785357142857143\n",
      "EPOCH: 337 TrainLoss: 0.12697875012059662 Accuracy: 0.37689285714285714\n",
      "EPOCH: 338 TrainLoss: 0.1267644838890467 Accuracy: 0.38057142857142856\n",
      "EPOCH: 339 TrainLoss: 0.12672292503773372 Accuracy: 0.37510714285714286\n",
      "EPOCH: 340 TrainLoss: 0.1265796822400453 Accuracy: 0.37685714285714284\n",
      "EPOCH: 341 TrainLoss: 0.12643310912562641 Accuracy: 0.3769642857142857\n",
      "EPOCH: 342 TrainLoss: 0.12646947422554386 Accuracy: 0.3789642857142857\n",
      "EPOCH: 343 TrainLoss: 0.12655212757712608 Accuracy: 0.377\n",
      "EPOCH: 344 TrainLoss: 0.12636511300201395 Accuracy: 0.3826428571428571\n",
      "EPOCH: 345 TrainLoss: 0.12625669312516655 Accuracy: 0.3811785714285714\n",
      "EPOCH: 346 TrainLoss: 0.1262773955486153 Accuracy: 0.37785714285714284\n",
      "EPOCH: 347 TrainLoss: 0.12623448000898413 Accuracy: 0.38460714285714287\n",
      "EPOCH: 348 TrainLoss: 0.1259672915100946 Accuracy: 0.3796428571428571\n",
      "EPOCH: 349 TrainLoss: 0.1260612772099614 Accuracy: 0.38557142857142856\n",
      "EPOCH: 350 TrainLoss: 0.12601628497558606 Accuracy: 0.3803214285714286\n",
      "EPOCH: 351 TrainLoss: 0.12589387734965313 Accuracy: 0.38389285714285715\n",
      "EPOCH: 352 TrainLoss: 0.1259000758264688 Accuracy: 0.38085714285714284\n",
      "EPOCH: 353 TrainLoss: 0.12568061679653034 Accuracy: 0.3822857142857143\n",
      "EPOCH: 354 TrainLoss: 0.12587692633386835 Accuracy: 0.38307142857142856\n",
      "EPOCH: 355 TrainLoss: 0.1255701049405775 Accuracy: 0.3792857142857143\n",
      "EPOCH: 356 TrainLoss: 0.12558704139380714 Accuracy: 0.3821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 357 TrainLoss: 0.1255796417674358 Accuracy: 0.38107142857142856\n",
      "EPOCH: 358 TrainLoss: 0.1253140236788599 Accuracy: 0.38010714285714287\n",
      "EPOCH: 359 TrainLoss: 0.12536487376722114 Accuracy: 0.3879642857142857\n",
      "EPOCH: 360 TrainLoss: 0.12525236273540785 Accuracy: 0.38392857142857145\n",
      "EPOCH: 361 TrainLoss: 0.12529911052083198 Accuracy: 0.38485714285714284\n",
      "EPOCH: 362 TrainLoss: 0.12511676543166844 Accuracy: 0.38642857142857145\n",
      "EPOCH: 363 TrainLoss: 0.12509067124603201 Accuracy: 0.38635714285714284\n",
      "EPOCH: 364 TrainLoss: 0.12501465993005106 Accuracy: 0.38707142857142857\n",
      "EPOCH: 365 TrainLoss: 0.12481084516544841 Accuracy: 0.3839642857142857\n",
      "EPOCH: 366 TrainLoss: 0.12501859674067878 Accuracy: 0.3891071428571429\n",
      "EPOCH: 367 TrainLoss: 0.12495979245179938 Accuracy: 0.38921428571428573\n",
      "EPOCH: 368 TrainLoss: 0.12475598955132892 Accuracy: 0.38653571428571426\n",
      "EPOCH: 369 TrainLoss: 0.12453980060425089 Accuracy: 0.38621428571428573\n",
      "EPOCH: 370 TrainLoss: 0.1246252772929604 Accuracy: 0.3836428571428571\n",
      "EPOCH: 371 TrainLoss: 0.12437400147600201 Accuracy: 0.3889285714285714\n",
      "EPOCH: 372 TrainLoss: 0.12440629306040833 Accuracy: 0.38789285714285715\n",
      "EPOCH: 373 TrainLoss: 0.12436452687899374 Accuracy: 0.3939285714285714\n",
      "EPOCH: 374 TrainLoss: 0.12427365264710588 Accuracy: 0.3883214285714286\n",
      "EPOCH: 375 TrainLoss: 0.1241084163724484 Accuracy: 0.3876428571428571\n",
      "EPOCH: 376 TrainLoss: 0.1239915773056304 Accuracy: 0.3904285714285714\n",
      "EPOCH: 377 TrainLoss: 0.12428405544715068 Accuracy: 0.39153571428571426\n",
      "EPOCH: 378 TrainLoss: 0.12415479310281781 Accuracy: 0.3909285714285714\n",
      "EPOCH: 379 TrainLoss: 0.12393438144053198 Accuracy: 0.3925\n",
      "EPOCH: 380 TrainLoss: 0.12405695674315055 Accuracy: 0.3915\n",
      "EPOCH: 381 TrainLoss: 0.12388763859878249 Accuracy: 0.3906428571428571\n",
      "EPOCH: 382 TrainLoss: 0.12402846591143882 Accuracy: 0.39289285714285715\n",
      "EPOCH: 383 TrainLoss: 0.12373191396024039 Accuracy: 0.38967857142857143\n",
      "EPOCH: 384 TrainLoss: 0.12377424620582557 Accuracy: 0.38917857142857143\n",
      "EPOCH: 385 TrainLoss: 0.12363612917775135 Accuracy: 0.39057142857142857\n",
      "EPOCH: 386 TrainLoss: 0.12361260252724161 Accuracy: 0.393\n",
      "EPOCH: 387 TrainLoss: 0.1234956712941135 Accuracy: 0.39425\n",
      "EPOCH: 388 TrainLoss: 0.12321820789412398 Accuracy: 0.3935\n",
      "EPOCH: 389 TrainLoss: 0.12344514332717946 Accuracy: 0.39271428571428574\n",
      "EPOCH: 390 TrainLoss: 0.1235421491107607 Accuracy: 0.39603571428571427\n",
      "EPOCH: 391 TrainLoss: 0.12330656485013465 Accuracy: 0.3959642857142857\n",
      "EPOCH: 392 TrainLoss: 0.12302246670779657 Accuracy: 0.39614285714285713\n",
      "EPOCH: 393 TrainLoss: 0.12321663751145046 Accuracy: 0.3986071428571429\n",
      "EPOCH: 394 TrainLoss: 0.12301631334437989 Accuracy: 0.4005\n",
      "EPOCH: 395 TrainLoss: 0.12301092393876481 Accuracy: 0.3906428571428571\n",
      "EPOCH: 396 TrainLoss: 0.12293027231274391 Accuracy: 0.39375\n",
      "EPOCH: 397 TrainLoss: 0.12281613636312631 Accuracy: 0.3906071428571429\n",
      "EPOCH: 398 TrainLoss: 0.12279918954148418 Accuracy: 0.3981071428571429\n",
      "EPOCH: 399 TrainLoss: 0.12275022160759468 Accuracy: 0.39489285714285716\n",
      "EPOCH: 400 TrainLoss: 0.1227587060767585 Accuracy: 0.39471428571428574\n",
      "EPOCH: 401 TrainLoss: 0.12282838697048046 Accuracy: 0.3927857142857143\n",
      "EPOCH: 402 TrainLoss: 0.12255149462556167 Accuracy: 0.3934642857142857\n",
      "EPOCH: 403 TrainLoss: 0.12253444125380057 Accuracy: 0.39582142857142855\n",
      "EPOCH: 404 TrainLoss: 0.12231000681258039 Accuracy: 0.3965714285714286\n",
      "EPOCH: 405 TrainLoss: 0.12250196589067036 Accuracy: 0.39435714285714285\n",
      "EPOCH: 406 TrainLoss: 0.12232121049337945 Accuracy: 0.39489285714285716\n",
      "EPOCH: 407 TrainLoss: 0.12238309137965708 Accuracy: 0.4005714285714286\n",
      "EPOCH: 408 TrainLoss: 0.12234171266469633 Accuracy: 0.39539285714285716\n",
      "EPOCH: 409 TrainLoss: 0.1218954715405699 Accuracy: 0.39982142857142855\n",
      "EPOCH: 410 TrainLoss: 0.12225289663863581 Accuracy: 0.3999642857142857\n",
      "EPOCH: 411 TrainLoss: 0.1219861096661801 Accuracy: 0.4044285714285714\n",
      "EPOCH: 412 TrainLoss: 0.1220230672379248 Accuracy: 0.39975\n",
      "EPOCH: 413 TrainLoss: 0.12204234123175862 Accuracy: 0.395\n",
      "EPOCH: 414 TrainLoss: 0.12188391624160554 Accuracy: 0.40085714285714286\n",
      "EPOCH: 415 TrainLoss: 0.12172701609515875 Accuracy: 0.4004642857142857\n",
      "EPOCH: 416 TrainLoss: 0.12159356009913393 Accuracy: 0.3972857142857143\n",
      "EPOCH: 417 TrainLoss: 0.1216835895968505 Accuracy: 0.4057857142857143\n",
      "EPOCH: 418 TrainLoss: 0.12189096796199192 Accuracy: 0.4020714285714286\n",
      "EPOCH: 419 TrainLoss: 0.12173137189181213 Accuracy: 0.4017142857142857\n",
      "EPOCH: 420 TrainLoss: 0.12169538864266713 Accuracy: 0.4010357142857143\n",
      "EPOCH: 421 TrainLoss: 0.12119591617854071 Accuracy: 0.3992857142857143\n",
      "EPOCH: 422 TrainLoss: 0.12159605728402577 Accuracy: 0.39939285714285716\n",
      "EPOCH: 423 TrainLoss: 0.12154523442548751 Accuracy: 0.4005\n",
      "EPOCH: 424 TrainLoss: 0.12135125652776439 Accuracy: 0.4044285714285714\n",
      "EPOCH: 425 TrainLoss: 0.12120360394890405 Accuracy: 0.39682142857142855\n",
      "EPOCH: 426 TrainLoss: 0.12118110318443859 Accuracy: 0.39753571428571427\n",
      "EPOCH: 427 TrainLoss: 0.12105455987357244 Accuracy: 0.4039642857142857\n",
      "EPOCH: 428 TrainLoss: 0.12120098560417708 Accuracy: 0.4014285714285714\n",
      "EPOCH: 429 TrainLoss: 0.12099957040093728 Accuracy: 0.4009285714285714\n",
      "EPOCH: 430 TrainLoss: 0.12082893151026816 Accuracy: 0.40414285714285714\n",
      "EPOCH: 431 TrainLoss: 0.12081901821380348 Accuracy: 0.4025357142857143\n",
      "EPOCH: 432 TrainLoss: 0.1210948217721662 Accuracy: 0.40664285714285714\n",
      "EPOCH: 433 TrainLoss: 0.12099680340359358 Accuracy: 0.4030714285714286\n",
      "EPOCH: 434 TrainLoss: 0.12077745085664675 Accuracy: 0.40539285714285717\n",
      "EPOCH: 435 TrainLoss: 0.12085220349269639 Accuracy: 0.40482142857142855\n",
      "EPOCH: 436 TrainLoss: 0.12050767407399295 Accuracy: 0.40314285714285714\n",
      "EPOCH: 437 TrainLoss: 0.12071661986747588 Accuracy: 0.4060714285714286\n",
      "EPOCH: 438 TrainLoss: 0.12046806029178707 Accuracy: 0.40460714285714283\n",
      "EPOCH: 439 TrainLoss: 0.12055692300254073 Accuracy: 0.4047142857142857\n",
      "EPOCH: 440 TrainLoss: 0.12045824660898616 Accuracy: 0.40667857142857144\n",
      "EPOCH: 441 TrainLoss: 0.12058492251866251 Accuracy: 0.40575\n",
      "EPOCH: 442 TrainLoss: 0.12033640826552021 Accuracy: 0.39975\n",
      "EPOCH: 443 TrainLoss: 0.12042932165312886 Accuracy: 0.41064285714285714\n",
      "EPOCH: 444 TrainLoss: 0.12009371618222538 Accuracy: 0.406\n",
      "EPOCH: 445 TrainLoss: 0.12010548057822562 Accuracy: 0.40460714285714283\n",
      "EPOCH: 446 TrainLoss: 0.1205007306288216 Accuracy: 0.40539285714285717\n",
      "EPOCH: 447 TrainLoss: 0.12026814602927614 Accuracy: 0.40689285714285717\n",
      "EPOCH: 448 TrainLoss: 0.12016206963001241 Accuracy: 0.41210714285714284\n",
      "EPOCH: 449 TrainLoss: 0.12023714810749352 Accuracy: 0.4077857142857143\n",
      "EPOCH: 450 TrainLoss: 0.11996588485366194 Accuracy: 0.40817857142857145\n",
      "EPOCH: 451 TrainLoss: 0.11993534203342697 Accuracy: 0.41235714285714287\n",
      "EPOCH: 452 TrainLoss: 0.11989899835825922 Accuracy: 0.4090714285714286\n",
      "EPOCH: 453 TrainLoss: 0.11958122652225538 Accuracy: 0.40660714285714283\n",
      "EPOCH: 454 TrainLoss: 0.11962008518636182 Accuracy: 0.4120714285714286\n",
      "EPOCH: 455 TrainLoss: 0.11966557021293653 Accuracy: 0.41132142857142856\n",
      "EPOCH: 456 TrainLoss: 0.1196942867292789 Accuracy: 0.40675\n",
      "EPOCH: 457 TrainLoss: 0.11969466260925078 Accuracy: 0.4047142857142857\n",
      "EPOCH: 458 TrainLoss: 0.1195417595666114 Accuracy: 0.4118928571428571\n",
      "EPOCH: 459 TrainLoss: 0.11953555763108634 Accuracy: 0.41175\n",
      "EPOCH: 460 TrainLoss: 0.11950392540073387 Accuracy: 0.411\n",
      "EPOCH: 461 TrainLoss: 0.11940429855302703 Accuracy: 0.40835714285714286\n",
      "EPOCH: 462 TrainLoss: 0.11944015769436472 Accuracy: 0.40917857142857145\n",
      "EPOCH: 463 TrainLoss: 0.11947028610891801 Accuracy: 0.409\n",
      "EPOCH: 464 TrainLoss: 0.1192427249875201 Accuracy: 0.40735714285714286\n",
      "EPOCH: 465 TrainLoss: 0.11915583254868055 Accuracy: 0.40664285714285714\n",
      "EPOCH: 466 TrainLoss: 0.11908455295203553 Accuracy: 0.4110714285714286\n",
      "EPOCH: 467 TrainLoss: 0.11927325792490519 Accuracy: 0.41546428571428573\n",
      "EPOCH: 468 TrainLoss: 0.11914026099656146 Accuracy: 0.41560714285714284\n",
      "EPOCH: 469 TrainLoss: 0.11899895633660028 Accuracy: 0.41075\n",
      "EPOCH: 470 TrainLoss: 0.11896432211967009 Accuracy: 0.41425\n",
      "EPOCH: 471 TrainLoss: 0.11895085371184362 Accuracy: 0.41260714285714284\n",
      "EPOCH: 472 TrainLoss: 0.11891715134113133 Accuracy: 0.41460714285714284\n",
      "EPOCH: 473 TrainLoss: 0.11870301305190392 Accuracy: 0.41496428571428573\n",
      "EPOCH: 474 TrainLoss: 0.11897878382910664 Accuracy: 0.41282142857142856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 475 TrainLoss: 0.11873197851845078 Accuracy: 0.41578571428571426\n",
      "EPOCH: 476 TrainLoss: 0.11886564699785722 Accuracy: 0.4145714285714286\n",
      "EPOCH: 477 TrainLoss: 0.1187764079435748 Accuracy: 0.4172142857142857\n",
      "EPOCH: 478 TrainLoss: 0.11863074265194443 Accuracy: 0.41432142857142856\n",
      "EPOCH: 479 TrainLoss: 0.11852268758386116 Accuracy: 0.4133928571428571\n",
      "EPOCH: 480 TrainLoss: 0.11874741773073794 Accuracy: 0.4120714285714286\n",
      "EPOCH: 481 TrainLoss: 0.11861859685266138 Accuracy: 0.41310714285714284\n",
      "EPOCH: 482 TrainLoss: 0.11858852394989021 Accuracy: 0.41625\n",
      "EPOCH: 483 TrainLoss: 0.1184171357121075 Accuracy: 0.4153928571428571\n",
      "EPOCH: 484 TrainLoss: 0.11847936989029581 Accuracy: 0.4115357142857143\n",
      "EPOCH: 485 TrainLoss: 0.11841453408665176 Accuracy: 0.40910714285714284\n",
      "EPOCH: 486 TrainLoss: 0.11820091967722014 Accuracy: 0.4119642857142857\n",
      "EPOCH: 487 TrainLoss: 0.11836261121268128 Accuracy: 0.4135357142857143\n",
      "EPOCH: 488 TrainLoss: 0.1182724253155679 Accuracy: 0.41810714285714284\n",
      "EPOCH: 489 TrainLoss: 0.11830039691631136 Accuracy: 0.4089642857142857\n",
      "EPOCH: 490 TrainLoss: 0.1179979224092997 Accuracy: 0.41564285714285715\n",
      "EPOCH: 491 TrainLoss: 0.11819472855533789 Accuracy: 0.41285714285714287\n",
      "EPOCH: 492 TrainLoss: 0.11802783960776989 Accuracy: 0.41760714285714284\n",
      "EPOCH: 493 TrainLoss: 0.11796811874122447 Accuracy: 0.4123928571428571\n",
      "EPOCH: 494 TrainLoss: 0.11801675423032006 Accuracy: 0.41417857142857145\n",
      "EPOCH: 495 TrainLoss: 0.11804815905143694 Accuracy: 0.41635714285714287\n",
      "EPOCH: 496 TrainLoss: 0.11784212391028327 Accuracy: 0.4152142857142857\n",
      "EPOCH: 497 TrainLoss: 0.11763719639820577 Accuracy: 0.41575\n",
      "EPOCH: 498 TrainLoss: 0.11792831356683764 Accuracy: 0.41867857142857146\n",
      "EPOCH: 499 TrainLoss: 0.11805780228410172 Accuracy: 0.42110714285714285\n",
      "EPOCH: 500 TrainLoss: 0.1176539553124177 Accuracy: 0.4200357142857143\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "model = MLP_regressor()\n",
    "# 学習率\n",
    "lr = 0.01\n",
    "# 学習エポック数\n",
    "n_epoch = 500\n",
    "# 正答率\n",
    "acc_list = []\n",
    "# n_epoch繰り返す\n",
    "for n in range(n_epoch):\n",
    "    # 訓練\n",
    "    perm=np.random.permutation(len(x_train))\n",
    "    x=np.maximum(x_train,x_train[perm])/255\n",
    "    t=np.maximum(t_train,t_train[perm])\n",
    "    y=model.forward(x)\n",
    "    train_loss=MSE(t,y)\n",
    "    model.backward(t,y)\n",
    "    model.optimize_GradientDecent(lr)\n",
    "    \n",
    "    # テスト\n",
    "    perm=np.random.permutation(len(x_test))\n",
    "    y = model.forward(np.maximum(x_test,x_test[perm])/255)\n",
    "    acc_list.append((np.sort(y.argsort(axis=1)[:,-2:]) == np.sort(np.maximum(t_test,t_test[perm]).argsort(axis=1)[:,-2:])).mean())\n",
    "    print(\"EPOCH:\",n+1,\"TrainLoss:\",train_loss,\"Accuracy:\",acc_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記プログラムだと、エポック500でも50％ほどだった。  \n",
    "精度を上げる(+時間がないので学習時間の短縮の)ために、勾配法をSGD、adamに変えてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr*grads[key]###### 問1-1 ######\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)        \n",
    "        self.iter += 1        \n",
    "        for key in params.keys():\n",
    "            self.m[key] = self.beta1*self.m[key]+(1-self.beta1)*grads[key]###### 問1-2-1 ######\n",
    "            self.v[key] = self.beta2*self.v[key]+(1-self.beta2)*grads[key]**2###### 問1-2-2 ######\n",
    "            m_unbias = self.m[key] / (1-self.beta1**self.iter)###### 問1-2-3 ######\n",
    "            v_unbias = self.v[key] / (1-self.beta2**self.iter)###### 問1-2-4 ######\n",
    "            params[key] -= self.lr * m_unbias / (np.sqrt(v_unbias) + 1e-7)\n",
    "            \n",
    "class mnistMultiLayerNet:\n",
    "    \"\"\"\n",
    "    layer0: 784 次元の入力\n",
    "    ↓ w1, b1 で線形結合\n",
    "    ↓ relu で活性化\n",
    "    layer1: 100 次元の隠れ層\n",
    "    ↓ w2, b2 で線形結合\n",
    "    ↓ relu で活性化\n",
    "    layer2: 100 次元の隠れ層\n",
    "    ↓ w3, b3 で線形結合\n",
    "    ↓ relu で活性化\n",
    "    layer3: 100 次元の隠れ層\n",
    "    ↓ w4, b4 で線形結合\n",
    "    ↓ relu で活性化\n",
    "    layer4: 100 次元の隠れ層\n",
    "    ↓ w5, b5 で線形結合\n",
    "    layer5: 10 次元の出力層\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.input_size = 784\n",
    "        self.output_size = 10\n",
    "        self.hidden_size_list = [100, 100, 100, 100]\n",
    "        self.all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        self.hidden_layer_num = len(self.hidden_size_list)\n",
    "        self.weight_decay_lambda =0\n",
    "        self.params = {}\n",
    "        self.layers = {}\n",
    "        self.grads = {}\n",
    "\n",
    "        # 重みとバイアスの初期化\n",
    "        for idx in range(1, len(self.all_size_list)):\n",
    "            self.params['w' + str(idx)] = np.random.randn(self.all_size_list[idx-1], self.all_size_list[idx]) * 0.085\n",
    "            self.params['b' + str(idx)] = np.zeros(self.all_size_list[idx], dtype=float)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.layers['layer0'] = x\n",
    "        for idx in range(1, len(self.all_size_list)):\n",
    "            w = self.params['w' + str(idx)]\n",
    "            b = self.params['b' + str(idx)]\n",
    "            x = self.layers['layer' + str(idx - 1)]\n",
    "            self.layers['layer' + str(idx)] = relu(np.dot(x, w) + b)\n",
    "        return self.layers['layer' + str(idx)]\n",
    "        \n",
    "\n",
    "    def loss(self, y, t):\n",
    "        return mse(t,y)\n",
    "    \n",
    "    def backward(self, t, y):\n",
    "        delta = (y - t) / t.shape[0]\n",
    "        self.grads['b5'] = np.sum(delta, axis=0)\n",
    "        self.grads['w5'] = np.dot(self.layers['layer4'].transpose(), delta)\n",
    "        # 誤差逆伝播\n",
    "        for idx in range(4, 0, -1):\n",
    "            delta = np.dot(delta, self.params['w' + str(idx + 1)].transpose())\n",
    "            delta = delta *  (self.layers['layer' + str(idx)] > 0)\n",
    "            self.grads['b' + str(idx)] = np.sum(delta, axis=0)\n",
    "            self.grads['w' + str(idx)] = np.dot(self.layers['layer'+str(idx - 1)].transpose(), delta)\n",
    "        return self.grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは通常のmnist。  \n",
    "そのままだと数字が大きすぎてエラー吐いたの、でmnistの最大値(255)で割って正規化した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 SGD Accuracy: 0.6396428571428572 ADAM Accuracy: 0.9390714285714286\n",
      "EPOCH: 2 SGD Accuracy: 0.7642142857142857 ADAM Accuracy: 0.946\n",
      "EPOCH: 3 SGD Accuracy: 0.8213571428571429 ADAM Accuracy: 0.9523571428571429\n",
      "EPOCH: 4 SGD Accuracy: 0.8499285714285715 ADAM Accuracy: 0.9477857142857142\n",
      "EPOCH: 5 SGD Accuracy: 0.867 ADAM Accuracy: 0.949\n",
      "EPOCH: 6 SGD Accuracy: 0.8801428571428571 ADAM Accuracy: 0.9562142857142857\n",
      "EPOCH: 7 SGD Accuracy: 0.8880714285714286 ADAM Accuracy: 0.9562857142857143\n",
      "EPOCH: 8 SGD Accuracy: 0.895 ADAM Accuracy: 0.9546428571428571\n",
      "EPOCH: 9 SGD Accuracy: 0.8994285714285715 ADAM Accuracy: 0.9557857142857142\n",
      "EPOCH: 10 SGD Accuracy: 0.9038571428571428 ADAM Accuracy: 0.9584285714285714\n",
      "EPOCH: 11 SGD Accuracy: 0.9077857142857143 ADAM Accuracy: 0.956\n",
      "EPOCH: 12 SGD Accuracy: 0.9107142857142857 ADAM Accuracy: 0.9588571428571429\n",
      "EPOCH: 13 SGD Accuracy: 0.9132142857142858 ADAM Accuracy: 0.9577142857142857\n",
      "EPOCH: 14 SGD Accuracy: 0.9157142857142857 ADAM Accuracy: 0.9582142857142857\n",
      "EPOCH: 15 SGD Accuracy: 0.9198571428571428 ADAM Accuracy: 0.9579285714285715\n",
      "EPOCH: 16 SGD Accuracy: 0.9233571428571429 ADAM Accuracy: 0.9621428571428572\n",
      "EPOCH: 17 SGD Accuracy: 0.9237857142857143 ADAM Accuracy: 0.96\n",
      "EPOCH: 18 SGD Accuracy: 0.925 ADAM Accuracy: 0.9581428571428572\n",
      "EPOCH: 19 SGD Accuracy: 0.9266428571428571 ADAM Accuracy: 0.9610714285714286\n",
      "EPOCH: 20 SGD Accuracy: 0.9276428571428571 ADAM Accuracy: 0.9592142857142857\n"
     ]
    }
   ],
   "source": [
    "#bn = mnistMultiLayerBatchNet()\n",
    "nobn = mnistMultiLayerNet()\n",
    "#adambn = mnistMultiLayerBatchNet()\n",
    "adamnobn = mnistMultiLayerNet()\n",
    "\n",
    "#bn_acc_list = []\n",
    "nobn_acc_list = []\n",
    "#adambn_acc_list = []\n",
    "adamnobn_acc_list = []\n",
    "\n",
    "sgd = SGD(lr = 0.01)\n",
    "adam = Adam(lr=0.01)\n",
    "\n",
    "# ミニバッチアルゴリズム\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(20):\n",
    "    # ランダムにミニバッチへ分割するために、インデックスをランダムに並び替える\n",
    "    perm = np.random.permutation(len(x_train))    \n",
    "    # batch_size ごとにデータを読み込んで学習させる\n",
    "    for idx in np.arange(0,len(perm),batch_size):\n",
    "        x = x_train[perm[idx:idx+batch_size]]/255\n",
    "        t = t_train[perm[idx:idx+batch_size]]\n",
    "\n",
    "        y = nobn.forward(x)\n",
    "        grads = nobn.backward(t,y)\n",
    "        sgd.update(nobn.params, grads)\n",
    "        \n",
    "        y = adamnobn.forward(x)\n",
    "        grads = adamnobn.backward(t, y)\n",
    "        adam.update(adamnobn.params,grads)\n",
    "\n",
    "    y_test = nobn.forward(x_test/255)\n",
    "    nobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    y_test = adamnobn.forward(x_test/255)\n",
    "    adamnobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n",
    "    print(\"EPOCH:\",epoch+1,\"SGD Accuracy:\",nobn_acc_list[-1],\"ADAM Accuracy:\",adamnobn_acc_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて重なったmnist。  \n",
    "2つの文字が重なるとパターンが一気に増えるので、エポック数も増やしてみた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 SGD Accuracy: 0.4362857142857143 ADAM Accuracy: 0.6698571428571428\n",
      "EPOCH: 2 SGD Accuracy: 0.5253571428571429 ADAM Accuracy: 0.6898928571428572\n",
      "EPOCH: 3 SGD Accuracy: 0.568 ADAM Accuracy: 0.7077142857142857\n",
      "EPOCH: 4 SGD Accuracy: 0.5925714285714285 ADAM Accuracy: 0.7057857142857142\n",
      "EPOCH: 5 SGD Accuracy: 0.6088214285714286 ADAM Accuracy: 0.70625\n",
      "EPOCH: 6 SGD Accuracy: 0.6251071428571429 ADAM Accuracy: 0.7251785714285715\n",
      "EPOCH: 7 SGD Accuracy: 0.6293928571428572 ADAM Accuracy: 0.716\n",
      "EPOCH: 8 SGD Accuracy: 0.6416428571428572 ADAM Accuracy: 0.7236071428571429\n",
      "EPOCH: 9 SGD Accuracy: 0.6395 ADAM Accuracy: 0.71625\n",
      "EPOCH: 10 SGD Accuracy: 0.6534285714285715 ADAM Accuracy: 0.7239642857142857\n",
      "EPOCH: 11 SGD Accuracy: 0.6611071428571429 ADAM Accuracy: 0.7351428571428571\n",
      "EPOCH: 12 SGD Accuracy: 0.6618571428571428 ADAM Accuracy: 0.7225714285714285\n",
      "EPOCH: 13 SGD Accuracy: 0.6620714285714285 ADAM Accuracy: 0.7178928571428571\n",
      "EPOCH: 14 SGD Accuracy: 0.661 ADAM Accuracy: 0.7295\n",
      "EPOCH: 15 SGD Accuracy: 0.6785 ADAM Accuracy: 0.7330714285714286\n",
      "EPOCH: 16 SGD Accuracy: 0.67275 ADAM Accuracy: 0.7386428571428572\n",
      "EPOCH: 17 SGD Accuracy: 0.6822857142857143 ADAM Accuracy: 0.7344642857142857\n",
      "EPOCH: 18 SGD Accuracy: 0.6786428571428571 ADAM Accuracy: 0.7232857142857143\n",
      "EPOCH: 19 SGD Accuracy: 0.6816071428571429 ADAM Accuracy: 0.73025\n",
      "EPOCH: 20 SGD Accuracy: 0.6845 ADAM Accuracy: 0.7264285714285714\n",
      "EPOCH: 21 SGD Accuracy: 0.6757142857142857 ADAM Accuracy: 0.7320357142857142\n",
      "EPOCH: 22 SGD Accuracy: 0.6881428571428572 ADAM Accuracy: 0.7198928571428571\n",
      "EPOCH: 23 SGD Accuracy: 0.696 ADAM Accuracy: 0.7171785714285714\n",
      "EPOCH: 24 SGD Accuracy: 0.6905714285714286 ADAM Accuracy: 0.7364642857142857\n",
      "EPOCH: 25 SGD Accuracy: 0.6883214285714285 ADAM Accuracy: 0.745\n",
      "EPOCH: 26 SGD Accuracy: 0.6992857142857143 ADAM Accuracy: 0.7240714285714286\n",
      "EPOCH: 27 SGD Accuracy: 0.6963571428571429 ADAM Accuracy: 0.7273214285714286\n",
      "EPOCH: 28 SGD Accuracy: 0.69425 ADAM Accuracy: 0.731\n",
      "EPOCH: 29 SGD Accuracy: 0.702 ADAM Accuracy: 0.7151785714285714\n",
      "EPOCH: 30 SGD Accuracy: 0.69875 ADAM Accuracy: 0.7405357142857143\n",
      "EPOCH: 31 SGD Accuracy: 0.7034285714285714 ADAM Accuracy: 0.7374285714285714\n",
      "EPOCH: 32 SGD Accuracy: 0.71275 ADAM Accuracy: 0.7345357142857143\n",
      "EPOCH: 33 SGD Accuracy: 0.7030357142857143 ADAM Accuracy: 0.7354285714285714\n",
      "EPOCH: 34 SGD Accuracy: 0.7123571428571429 ADAM Accuracy: 0.7409285714285714\n",
      "EPOCH: 35 SGD Accuracy: 0.7038214285714286 ADAM Accuracy: 0.7290714285714286\n",
      "EPOCH: 36 SGD Accuracy: 0.7107142857142857 ADAM Accuracy: 0.7303928571428572\n",
      "EPOCH: 37 SGD Accuracy: 0.7094285714285714 ADAM Accuracy: 0.7238214285714286\n",
      "EPOCH: 38 SGD Accuracy: 0.7153214285714286 ADAM Accuracy: 0.7305357142857143\n",
      "EPOCH: 39 SGD Accuracy: 0.7118571428571429 ADAM Accuracy: 0.7356071428571429\n",
      "EPOCH: 40 SGD Accuracy: 0.7096071428571429 ADAM Accuracy: 0.7280714285714286\n",
      "EPOCH: 41 SGD Accuracy: 0.7110357142857143 ADAM Accuracy: 0.7288571428571429\n",
      "EPOCH: 42 SGD Accuracy: 0.7146785714285714 ADAM Accuracy: 0.7328214285714286\n",
      "EPOCH: 43 SGD Accuracy: 0.7127142857142857 ADAM Accuracy: 0.7344285714285714\n",
      "EPOCH: 44 SGD Accuracy: 0.7144642857142857 ADAM Accuracy: 0.7315714285714285\n",
      "EPOCH: 45 SGD Accuracy: 0.7150357142857143 ADAM Accuracy: 0.7330357142857142\n",
      "EPOCH: 46 SGD Accuracy: 0.72125 ADAM Accuracy: 0.73475\n",
      "EPOCH: 47 SGD Accuracy: 0.7221071428571428 ADAM Accuracy: 0.7329285714285714\n",
      "EPOCH: 48 SGD Accuracy: 0.7238571428571429 ADAM Accuracy: 0.7320714285714286\n",
      "EPOCH: 49 SGD Accuracy: 0.7283214285714286 ADAM Accuracy: 0.7445357142857143\n",
      "EPOCH: 50 SGD Accuracy: 0.7282857142857143 ADAM Accuracy: 0.72725\n"
     ]
    }
   ],
   "source": [
    "#bn = mnistMultiLayerBatchNet()\n",
    "nobn = mnistMultiLayerNet()\n",
    "#adambn = mnistMultiLayerBatchNet()\n",
    "adamnobn = mnistMultiLayerNet()\n",
    "\n",
    "#bn_acc_list = []\n",
    "nobn_acc_list = []\n",
    "#adambn_acc_list = []\n",
    "adamnobn_acc_list = []\n",
    "\n",
    "sgd = SGD(lr = 0.01)\n",
    "adam = Adam(lr=0.01)\n",
    "\n",
    "# ミニバッチアルゴリズム\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(50):\n",
    "    # 訓練\n",
    "    # ランダムにミニバッチへ分割するために、インデックスをランダムに並び替える\n",
    "    perm = np.random.permutation(len(x_train))\n",
    "    perm1 = np.random.permutation(len(x_train))    \n",
    "    # batch_size ごとにデータを読み込んで学習させる\n",
    "    for idx in np.arange(0,len(perm),batch_size):\n",
    "        x = np.maximum(x_train[perm[idx:idx+batch_size]],x_train[perm1[idx:idx+batch_size]])/255\n",
    "        t = np.maximum(t_train[perm[idx:idx+batch_size]],t_train[perm1[idx:idx+batch_size]])\n",
    "        #SGDによる学習\n",
    "        y = nobn.forward(x)\n",
    "        grads = nobn.backward(t,y)\n",
    "        sgd.update(nobn.params, grads)\n",
    "        #ADAMによる学習\n",
    "        y = adamnobn.forward(x)\n",
    "        grads = adamnobn.backward(t, y)\n",
    "        adam.update(adamnobn.params,grads)\n",
    "    # テスト\n",
    "    perm=np.random.permutation(len(x_test))\n",
    "    y = nobn.forward(np.maximum(x_test,x_test[perm])/255)\n",
    "    nobn_acc_list.append((np.sort(y.argsort(axis=1)[:,-2:]) == np.sort(np.maximum(t_test,t_test[perm]).argsort(axis=1)[:,-2:])).mean())\n",
    "    y = adamnobn.forward(np.maximum(x_test,x_test[perm])/255)\n",
    "    adamnobn_acc_list.append((np.sort(y.argsort(axis=1)[:,-2:]) == np.sort(np.maximum(t_test,t_test[perm]).argsort(axis=1)[:,-2:])).mean())\n",
    "    print(\"EPOCH:\",epoch+1,\"SGD Accuracy:\",nobn_acc_list[-1],\"ADAM Accuracy:\",adamnobn_acc_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論\n",
    "色々試してみたが、80%の壁を超えるのは難しかった。\n",
    "下のお遊びで試してみた感じでは、「1と9」が、人間の目でも「8」に見えたりするので、そういったところが要因だと感じた。  \n",
    "そういう意味では、「ディープラーニングはデータセットが肝」というのがよく分かった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下お遊び。  \n",
    "結構視覚的にわかるので、会社とかでのデモに使えそう。  \n",
    "for文使えば5文字くらい連続して表示できるけど、下に長くなるので結構見づらい・・・。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [0. 0. 1. 0. 0. 0. 1. 0. 0. 0.] ⇒ [2 6]\n",
      "Label: [ 78.48403662 -54.71331616 150.28095181  25.7871618  -40.27301211\n",
      "  -7.3247983   92.51939669 -16.76108116  -7.64188839 -40.34348239] ⇒ [2 6]\n"
     ]
    }
   ],
   "source": [
    "rand0 = np.random.randint(0,len(x_train))\n",
    "rand1 = np.random.randint(0,len(x_train))\n",
    "\n",
    "x_train_overlap = np.maximum(x_train[rand0],x_train[rand1])\n",
    "t_train_overlap = np.maximum(t_train[rand0],t_train[rand1])\n",
    "plt.gray()\n",
    "plt.imshow(x_train_overlap.reshape((28,28)))\n",
    "plt.show()\n",
    "y=model.forward(x_train_overlap)\n",
    "print(\"Label:\",t_train_overlap,\"⇒\",np.sort(t_train_overlap.argsort()[-2:]))\n",
    "print(\"Label:\",y,\"⇒\",np.sort(y.argsort()[-2:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
